<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>DEVXDAO PRUNING GRANT 37</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="technology/technology.html">Technology</a></li><li class="chapter-item expanded "><a href="solution/solution_.html"><strong aria-hidden="true">1.</strong> Solution</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="solution/solution.html"><strong aria-hidden="true">1.1.</strong> Solution</a></li><li class="chapter-item expanded "><a href="solution/architecture.html"><strong aria-hidden="true">1.2.</strong> Architecture</a></li><li class="chapter-item expanded "><a href="solution/qsss_benefits.html"><strong aria-hidden="true">1.3.</strong> Storage System Benefits</a></li></ol></li><li class="chapter-item expanded "><a href="technology/technology.html"><strong aria-hidden="true">2.</strong> Technology</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="technology/qsss/qsss_home.html"><strong aria-hidden="true">2.1.</strong> Quantum Safe Storage</a></li><li class="chapter-item expanded "><a href="technology/qsss/qss_algorithm.html"><strong aria-hidden="true">2.2.</strong> Quantum Safe Algo</a></li><li class="chapter-item expanded "><a href="technology/primitives/compute/compute.html"><strong aria-hidden="true">2.3.</strong> Compute</a></li></ol></li><li class="chapter-item expanded "><a href="research.html"><strong aria-hidden="true">3.</strong> Research</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="industry/the_problem.html"><strong aria-hidden="true">3.1.</strong> The Problem</a></li><li class="chapter-item expanded "><a href="industry/analysis.html"><strong aria-hidden="true">3.2.</strong> Analysis</a></li><li class="chapter-item expanded "><a href="research/casper_data_structure.html"><strong aria-hidden="true">3.3.</strong> CasperLabs Datastructure</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">DEVXDAO PRUNING GRANT 37</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <p><img src="technology/img/tech_overview.png" alt="" /></p>
<h1 id="technology"><a class="header" href="#technology">Technology</a></h1>
<p><img src="technology/img/tech_architecture1.jpg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="solution"><a class="header" href="#solution">Solution</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="suggestion-solution"><a class="header" href="#suggestion-solution">Suggestion Solution</a></h1>
<p>Implementing good enough pruning for Casper Labs is not a trivial task.</p>
<p>We believe that we have to put it in 2 phases, 1 phase is for research and proof of concept which can be used generically and proves the concept of a global pruning layer.</p>
<p>The 2nd phase should be for another grant of other collaboration effort to put it inside the storage engine of Casper Blockchain and work closely together with the engineers of Casperlabs.</p>
<h2 id="phase-1-solution-this-grant"><a class="header" href="#phase-1-solution-this-grant">Phase 1 Solution (this grant)</a></h2>
<p>A pruning DB, writing on global level to hundreds of backends in such a way the performance stays high, the data cannot be corrupted, the data can be self-healed when needed. Demonstrate how a pruning DB can be a super efficient backend storage layer for a blockchain like Casper Labs.</p>
<p><strong>Solution see <a href="solution/architecture.html">architecture</a></strong>.</p>
<h3 id="proof-points"><a class="header" href="#proof-points">Proof Points</a></h3>
<ul>
<li>create a global accesible data lake for a blockchain which is deployed in 100 locations and only 20 nodes are needed to retrieve the data.</li>
<li>Prove that 80 nodes can be offline and data can still be retrieved.</li>
<li>Prove that 100 TB of data can be stored this way using 1 Pruning DB.</li>
<li>Make test scripts which show how performance is high for the key value stor and stays high independent of size of DB. We will test with 100 TB.</li>
<li>Show how multiple Prune DB's at the same DB can read the data when not in cache from the same data lake.</li>
<li>prove that 100 MB/sec can be achieved towards backend (if enough bandwidth available)</li>
<li>show how data gets encrypted and distributed in such a way that even for a serious hacker it would be hard to do something with the data.</li>
<li>this effort goes together with another grant which is the deployment of such a system on top of TFGrid in all scalability and with a consensus driven deployment mechanism.</li>
<li>All the code delivered is opensource.</li>
</ul>
<h2 id="future-grants"><a class="header" href="#future-grants">Future Grants</a></h2>
<h3 id="integration-in-casperlabs-blockchain-backend"><a class="header" href="#integration-in-casperlabs-blockchain-backend">Integration in Casperlabs Blockchain Backend</a></h3>
<ul>
<li>show how seamless pruning can be implemented in POC using the blockchain engine</li>
<li>this is a serious job and will require quite some effort to make sure it happens in the safest way and gets well tested.</li>
<li>this will require low level integration so that the data paths dont change to much.</li>
<li>Use the Blockchain layer to also prove authenticity of the pruning storage layer (is an extra check, not strictly needed, but might give extra confidence to community).</li>
</ul>
<h3 id="production-readyness--testing"><a class="header" href="#production-readyness--testing">Production Readyness &amp; Testing</a></h3>
<p>Testing this new pruning layer in all its facets on performance and reliability level.</p>
<h3 id="master-slave-solution-for-pruning-db"><a class="header" href="#master-slave-solution-for-pruning-db">Master-Slave Solution for pruning DB</a></h3>
<ul>
<li>Make Pruning DB Redundant (active-active or active-passive)</li>
<li>Show how there is never a reliability issue, create good test cases to show this redundancy.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="architecture-this-grant"><a class="header" href="#architecture-this-grant">Architecture (this grant)</a></h2>
<pre class="mermaid">graph TD
    A[Blockchain Engine] --&gt;|Offload Data| B(Prune DB)
    B --&gt;|WRITE PATH| C{TF FLECC Codec}
    C --&gt;|One| D[NODE 1]
    C --&gt;|Two| E[NODE 2]
    C --&gt;|Three| F[NODE 3]
    C --&gt;|Hundred| G[... 100 NODES]
</pre>
<p>For write path we write to e.g. 100 nodes, all over the world. This amount is configurable and also the storage policy used.</p>
<p>Default we could use like 20 + 80, means min 20 nodes are needed to re-create the original. This means upto 60 nodes can be lost before data is lost.</p>
<h2 id="the-prune-db"><a class="header" href="#the-prune-db">The Prune DB</a></h2>
<p>Is a fast key value stor which can be used as backend for the storage engine of the blockchain for pruning support.</p>
<p>This key value stor will write in local DB's (storage containers see further) which are configurable in size and can be possitioned on storage system of choice, we recommend SSD.</p>
<p>The storage containers are cacheable, once written they can be removed, if data is needed from a contaier it will be fetched back from the FLECC CODEC in a redundant way. This will make the Prune DB caching aware and can support petabytes easily.</p>
<pre class="mermaid">graph TD
    A[Blockchain Engine] --&gt;|Offload Data| B(Prune DB)
    B --&gt;|Data| C(Storage Container 100 MB)
    B --&gt; D(Storage Container 100 MB)
    B --&gt; E(Storage Container 100 MB)
    B --&gt; X(metadate engine)
    X --&gt; F(Metadata DB 1 MB)
    X --&gt; G(Metadata DB 1 MB)
    X --&gt; H(Metadata DB 1 MB)

</pre>
<p>Above provides for a very scalable system, the metadata DB's and DATA DB's get offloaded using the FLECC Codec towards potentially thousands of nodes on the backend.</p>
<h2 id="self-healing-this-grant"><a class="header" href="#self-healing-this-grant">Self Healing (this grant)</a></h2>
<p>The system should be full self healing, this means if nodes get offline, or there is data rot (corrupted data), the data needs to be corrected automatically and redistributed so that we get back to the original health of the data in line with original policy.</p>
<h2 id="corruption-proof"><a class="header" href="#corruption-proof">Corruption Proof</a></h2>
<p>Data should not be able to get corrupted, once corruption occurs because of network or storage subsystem the codec needs to be able to recover it right away.</p>
<h2 id="the-write-path-is-redundant"><a class="header" href="#the-write-path-is-redundant">The write path is redundant</a></h2>
<pre class="mermaid">graph TD
    A[Blockchain Engine] --&gt;|Offload Data| B(Prune DB)
    B --&gt;|RAFT CONSENSUS| C{TF FLECC Codec}
    B --&gt;|Only 1 MASTER| D{TF FLECC Codec}
    B --&gt;|CONSENSUS| E{TF FLECC Codec}
    B --&gt;|CONSENSUS| F{TF FLECC Codec}
    B --&gt;|UPTO 9 is ok| G{TF FLECC Codec}

</pre>
<p>The Consensus layer for write ath will be based on Tendermint (or alternative if CasperLabs has suggestions).</p>
<p>The Pruning DB creates storage containers of 1-100 MB in size (configurable). Each Storage Container gets put in separate file if a configurable time interval got passed or a certain size for the storage container, default 100 MB.</p>
<h2 id="redundancy-for-prude-db"><a class="header" href="#redundancy-for-prude-db">Redundancy for Prude DB</a></h2>
<p>We will not make the Pruning DB active-active but this can be done as part of the next grant. For now the pruning DB will write the storage containers with data and metadata. If the node on which we do the activiation of the pruning dies then that pruning action will have to be restarted.</p>
<p>Active-Passive clustering can be build inside the Pruding DB.</p>
<pre class="mermaid">graph TD
    A[Blockchain Engine] --&gt;|Offload Data| B(Master Prune DB)
    B --&gt; |Synchronous need good network| C(Secondary Prune DB)
    B --&gt; D(Secondary Prune DB)

</pre>
<p>Only one pruning DB is active, if the master is down, a Raft mechanism might be used (next grant) to define which DB becomes the new master.</p>
<h2 id="each-zeros-node-can-store-500-tb"><a class="header" href="#each-zeros-node-can-store-500-tb">Each Zeros-Node can store 500 TB.</a></h2>
<pre class="mermaid">graph TD
    F{TF FLECC Codec} --&gt; A(NODE 1 HD ZDB)
    F{TF FLECC Codec} --&gt; B(NODE 1 HD ZDB)
    F{TF FLECC Codec} --&gt; C(NODE 1 HD ZDB)

</pre>
<p>100+ ZDB's can be used by 1 codec, the codec will make sure that data is spread out good enough.</p>
<h2 id="read-path"><a class="header" href="#read-path">Read Path</a></h2>
<p>The read path is different and redundant by design. Thanks to the FLECC Codec we can read data from hundred of sources and this can happen by many readers at the same time.</p>
<pre class="mermaid">graph TD
    P1(Prune DB US) ---|READ| F
    P2(Prune DB CH) ---|READ| F
    P3(Prune DB DUBAI) ---|READ| F
    P4(Prune DB TORONTO) ---|READ| F
    F{TF FLECC Codec} --- A(NODE 1 DUBAI HD ZDB)
    F --- B(NODE 2 BE HD ZDB)
    F --- C(NODE 3 US HD ZDB)

</pre>
<p>This leads to the fastest possible way how to retrieve data so the prune DB can retrieve the data it needs.</p>
<p>Once a storage container is retrieved it will stay in cache untill no longer needed.</p>
<p>The caching layer needs to be intelligent to delete the right files as much as possible.</p>
<h2 id="read-optimization-grant-future"><a class="header" href="#read-optimization-grant-future">Read Optimization (Grant Future)</a></h2>
<p>We can implement a system which will allow a faster retrievel, in the suggested system we only need to retrieve 20 files from the 100 ZDB's (backend DB's), what if the system would be smart enough to figure out which files to retrieve first and depending who answers fastest the data can be rebuild to have back the original data. It could work with e.g 50 of the 100, the first 20 who deliver allow the data to be retrieved, this would dramaically speed up retrieval and data would come from latency locations close by.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quantum-safe-storage-system-benefits"><a class="header" href="#quantum-safe-storage-system-benefits">Quantum Safe Storage System Benefits</a></h1>
<p>ThreeFold would start from our existin QSSS (Quantym Safe Storage System) to deliver what Casper Labs requires, we would use and extend our existing codebase.</p>
<p>QSFS stands for Quantum-Safe File System. It is a redundant storage system, which can store petabytes of information.</p>
<p>Unique features are : </p>
<ul>
<li>Unlimited scalable (many petabytes)</li>
<li>Data is spread over many devices owned by different, independent hardware owners called farmers. Together, these farmers provide the capacity to a hardware grid, call the ThreeFold Grid. </li>
<li>Dispersion over multiple sites ensures 100% privacy of the data, as no farmer knows what the data is about (zero knowledge storage system). Even a quantum computer cannot decrypt the data on a node, as one node contains insufficient information to unambiguously recreate the authentic data.</li>
<li>Data can’t be lost: there is a protection for datarot, data will auto-repair.</li>
<li>Data is append-only and immutable by design of the protocol, so it’s fit for storing ledger history. Even with sites going down, data is not lost with ‘operational’ backup nodes in place.</li>
<li>Up to 10x more efficient than storing on classic storage cloud systems. Overhead of about 20% is sufficient to have a secure archive.</li>
<li>Self-healing: when node or disk lost, storage system can get back to original redundancy level.</li>
<li>Helps with compliance to privacy regulations like GDPR (as the hosting facility has no view on what is stored, information is encrypted and incomplete).</li>
<li>Hybrid : can be installed onsite, public, private, ...</li>
<li>Read-write caching on encoding node (the front end).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p><img src="technology/img/tech_overview.png" alt="" /></p>
<h1 id="technology-1"><a class="header" href="#technology-1">Technology</a></h1>
<p><img src="technology/img/tech_architecture1.jpg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><!-- ![](img/qsss_intro_.jpg) -->
<p>i<img src="technology/qsss/img/qsss_intro.png" alt="" /></p>
<h1 id="quantum-safe-storage-system"><a class="header" href="#quantum-safe-storage-system">Quantum Safe Storage System</a></h1>
<p>Imagine a storage system with the following benefits</p>
<blockquote>
<p>This is not a dream but does exist and is the underpinning of the TFGrid.</p>
</blockquote>
<p>Our storage architecture follows the true peer-to-peer design of the TF grid. Any participating node only stores small incomplete parts of objects (files, photos, movies, databases...) by offering a slice of the present (local) storage devices. Managing the storage and retrieval of all of these distributed fragments is done by a software that creates development or end-user interfaces for this storage algorithm. We call this '<strong>dispersed storage</strong>'.</p>
<p><img src="technology/qsss/img/qsss_intro_0_.jpg" alt="" /></p>
<p>Peer2peer provides the unique proposition of selecting storage providers that match your application and service of business criteria. For example, you might be looking to store data for your application in a certain geographic area (for governance and compliance) reasons. Also, you might want to use different &quot;storage policies&quot; for different types of data. Examples are live versus archived data. All of these uses cases are possible with this storage architecture and could be built by using the same building blocks produced by farmers and consumed by developers or end-users.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quantum-safe-storage-algoritm"><a class="header" href="#quantum-safe-storage-algoritm">Quantum Safe Storage Algoritm</a></h1>
<p><img src="technology/qsss/img/tf_banner_grid_.jpg" alt="" /></p>
<p>The Quantum Safe Storage Algorithm is the heart of the Storage engine.  The storage engine takes the original data objects and creates data part descriptions that it stores over many virtual storage devices (ZDB/s)</p>
<p><img src="technology/qsss/../img/.jpg" alt="" /></p>
<p>Data gets stored over multiple ZDB's in such a way that data can never be lost.</p>
<p>Unique features</p>
<ul>
<li>data always append, can never be lost</li>
<li>even a quantum computer cannot decrypt the data</li>
<li>is spread over multiple sites, sites can be lost, data will still be available</li>
<li>protects for <a href="technology/qsss/datarot">datarot</a></li>
</ul>
<h3 id="why"><a class="header" href="#why">Why</a></h3>
<p>Today we produce more data than ever before. We could not continue to make full copies of data to make sure it is stored reliably. This will simply not scale. We need to move from securing the whole dataset to securing all the objects that make up a dataset.</p>
<p>ThreeFold is using space technology to store data (fragments) over multiple devices (physical storage devices in 3Nodes). The solution does not distribute and store parts of an object (file, photo, movie...) but describes the part of an object. This could be visualized by thinking of it as equations.</p>
<h3 id="details"><a class="header" href="#details">Details</a></h3>
<p>Let a,b,c,d.... be the parts of that original object. You could create endless unique equations using these parts. A simple example: let's assume we have 3 parts of original objects that have the following values:</p>
<pre><code>a=1
b=2
c=3
</code></pre>
<p>(and for reference that part of real-world objects is not a simple number like <code>1</code> but a unique digital number describing the part, like the binary code for it <code>110101011101011101010111101110111100001010101111011.....</code>). With these numbers we could create endless amounts of equations:</p>
<pre><code>1: a+b+c=6
2: c-b-a=0
3: b-c+a=0
4: 2b+a-c=2
5: 5c-b-a=12
......
</code></pre>
<p>Mathematically we only need 3 to describe the content (=value) of the fragments. But creating more adds reliability. Now store those equations distributed (one equation per physical storage device) and forget the original object. So we no longer have access to the values of a, b, c and see and we just remember the locations of all the equations created with the original data fragments. Mathematically we need three equations (any 3 of the total) to recover the original values for a, b or c. So do a request to retrieve 3 of the many equations and the first 3 to arrive are good enough to recalculate the original values. Three randomly retrieved equations are:</p>
<pre><code>5c-b-a=12
b-c+a=0
2b+a-c=2
</code></pre>
<p>And this is a mathematical system we could solve:</p>
<ul>
<li>First: <code>b-c+a=0 -&gt; b=c-a</code></li>
<li>Second: <code>2b+a-c=2 -&gt; c=2b+a-2 -&gt; c=2(c-a)+a-2 -&gt; c=2c-2a+a-2 -&gt; c=a+2</code></li>
<li>Third: <code>5c-b-a=12 -&gt; 5(a+2)-(c-a)-a=12 -&gt; 5a+10-(a+2)+a-a=12 -&gt; 5a-a-2=2 -&gt; 4a=4 -&gt; a=1</code></li>
</ul>
<p>Now that we know <code>a=1</code> we could solve the rest <code>c=a+2=3</code> and <code>b=c-a=2</code>. And we have from 3 random equations regenerated the original fragments and could now recreate the original object. </p>
<p>The redundancy and reliability in such system comes in the form of creating (more than needed) equations and storing them. As shown these equations in any random order could recreate the original fragments and therefore
redundancy comes in at a much lower overhead.</p>
<h3 id="example-of-164"><a class="header" href="#example-of-164">Example of 16/4</a></h3>
<p><img src="technology/qsss/img/quantumsafe_storage_algo.jpg" alt="" /></p>
<p>Each object is fragmented into 16 parts. So we have 16 original fragments for which we need 16 equations to mathematically describe them. Now let's make 20 equations and store them dispersedly on 20 devices. To recreate the original object we only need 16 equations, the first 16 that we find and collect which allows us to recover the fragment and in the end the original object. We could lose any 4 of those original 20 equations.</p>
<p>The likelihood of losing 4 independent, dispersed storage devices at the same time is very low. Since we have continuous monitoring of all of the stored equations, we could create additional equations immediately when one of them is missing, making it an auto-regeneration of lost data and a self-repairing storage system. The overhead in this example is 4 out of 20 which is a mere <strong>20%</strong> instead of (up to) <strong>400%.</strong></p>
<h3 id="content-distribution-policy-1050"><a class="header" href="#content-distribution-policy-1050">Content distribution Policy (10/50)</a></h3>
<p>This system can be used as backend for content delivery networks.</p>
<p>Imagine a movie being stored on 60 locations from which we can loose 50 at the same time.</p>
<p>If someone now wants to download the data the first 10 locations who answer fastest will provide enough of the data parts to allow the data to be rebuild.</p>
<p>The overhead here is much more compared to previous example but stil order of magnitude lower compared to other cdn systems.</p>
<h3 id="datarot"><a class="header" href="#datarot">Datarot</a></h3>
<blockquote>
<p>Datarot cannot happen on this storage system</p>
</blockquote>
<p>Fact that data storage degrades over time and becomes unreadable, on e.g. a harddisk.
The storage system provided by ThreeFold intercepts this silent data corruption, making that it can pass by unnotified.</p>
<blockquote>
<p>see also https://en.wikipedia.org/wiki/Data_degradation</p>
</blockquote>
<h3 id="zero-knowledge-proof"><a class="header" href="#zero-knowledge-proof">Zero Knowledge Proof</a></h3>
<p>The quantum save storage system is zero knowledge proof compliant. The storage system is made up / split into 2 components:  The actual storage devices use to store the data (ZDB's) and the Quantum Safe Storage engine. </p>
<p><img src="technology/qsss/img/qss_system.jpg" alt="" /></p>
<p>The zero proof knowledge compliancy comes from the fact the all the physical storage nodes (3nodes) can proof that they store a valid part of what data the quantum safe storage engine (QSSE) has stored on multiple independent devices.  The QSSE can validate that all the QSSE storage devices have a valid part of the original information.  The storage devices however have no idea what the original stored data is as they only have a part (description) of the origina data and have no access to the original data part or the complete origal data objects.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="tfgrid-compute-layer"><a class="header" href="#tfgrid-compute-layer">TFGrid Compute Layer</a></h2>
<p><img src="technology/primitives/compute/img/tfgrid_compute_.jpg" alt="" /></p>
<p>We are more than just Container or VM technology, see <a href="technology/primitives/compute/../../primitives/compute/beyond_containers.html">our Beyond Container Document</a>.</p>
<p>For more information see <a href="technology/primitives/compute/../../zos/zos_toc.html">ZeroOS</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="research"><a class="header" href="#research">Research</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-problem-forever-growing-data"><a class="header" href="#the-problem-forever-growing-data">The problem: Forever growing data</a></h1>
<p>Definition from the <a href="https://en.wikipedia.org/wiki/Blockchain">wiki</a>:
<strong>&quot;A blockchain is a growing list of records, called blocks, that are linked together using cryptography. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a Merkle tree). The timestamp proves that the transaction data existed when the block was published in order to get into its hash. As blocks each contain information about the block previous to it, they form a chain, with each additional block reinforcing the ones before it. Therefore, blockchains are resistant to modification of their data because once recorded, the data in any given block cannot be altered retroactively without altering all subsequent blocks.</strong>&quot; </p>
<p>Therefore blockchains by design create and carry historical data forever. The nature of a blockchain is that it stores data immutably and in an always-append manner. So from the moment of inception of a blockchain (the &quot;genesis&quot; block) to the current point in time (the latest block) all data (blocks) ever created need to be stored on <em>and</em> available on a sufficient set of (full) nodes to ensure the life of the chain.</p>
<p>This has profound implications for successful blockchains, regardless of their purpose. Whether the blockchain is a pure value transaction blockchain or it is a smart contract system with additional data storage requirements, it is all compounding to a forever growing storage requirement. To the extent that the data grows faster than the storage purchasing power of node operators, this becomes a centralizing force as fewer network participants can afford to host the whole chain.</p>
<p>There are two main categories of blockchains, each with specific benefits and disadvantages: permission<strong>less</strong> and permission<strong>ed</strong> blockchains. The first allows anyone to take part and start contributing to the network, the latter having a (central) authority that agrees who can participate and who not. Both types of blockchains have the same challenge with regards to data storage: it is a forever growing chain of blocks which increases the storage needs of participating nodes indefinitely, but pruning solutions are very different because of the following:</p>
<ul>
<li>in a permission<strong>less</strong> blockchain anyone can choose to participate to help to run and secure the blockchain by operating a blockchain node.  Consensus and trust is based on an algorithm, not on any central authority.</li>
<li>in a permission<strong>ed</strong> blockchain there is a central authority of some sort that allows blockchain nodes to be added to the network.  The central authority provides trust and to a degree consensus.</li>
</ul>
<p>So the blockchain pruning challenge is many times larger for permission<strong>less</strong> chains then for permission<strong>ed</strong> chains.</p>
<h2 id="evidence-of-the-problem-statement"><a class="header" href="#evidence-of-the-problem-statement">Evidence of the problem statement</a></h2>
<p>Overview (not exhaustive) of what the current blockchains experience in terms of data growth and the challenges that come with it at the current state of play of blockchain projects:</p>
<ul>
<li><a href="https://developers.stellar.org/docs/run-core-node/prerequisites/">Stellar</a></li>
<li><a href="https://decrypt.co/24779/ethereum-archive-nodes-now-take-up-4-terabytes-of-space">Ethereum</a></li>
<li><a href="https://101blockchains.com/blockchain-size/">blockchain size article</a></li>
</ul>
<p>Reading through these articles we can find chain size growth figures of 50x over a period of 2 years. And with the exponential growth and adoption of blockchain technologies this is a real problem that lies ahead of us which is not going to be solved by the rate of which hardware is improving and becoming more affordable.</p>
<p>|</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="analysis"><a class="header" href="#analysis">Analysis</a></h1>
<p>The current solutions to the forever growing data storage requirements to not tackle the real problem: How can we prune a (any) blockchain protocol without putting limitations and restrictions on the protocol itself but provide a storage utility that is trustworthy and builds on the principles provided by the blockchain protocol without changing them?</p>
<h2 id="current-solutions-to-forever-growing-data-in-blockchains"><a class="header" href="#current-solutions-to-forever-growing-data-in-blockchains">Current solutions to forever growing data in blockchains</a></h2>
<p>A non-exhaustive list of ways to deal with the data growth issue is:</p>
<ul>
<li><em>Compress data</em>: keep storing data as is but lower the impact / footprint of it.</li>
<li><em>Lower the number of full nodes in a blockchain</em>: make the number of nodes smaller in order to have less copies of the data distributed which lowers the synchronization requirements (to retain a high transaction speed) and limit the overall hardware needed to operate the full nodes.  This can be done by staking mechanisms and creating a set number of full nodes available in a chain.</li>
<li><em>Create a hierarchy of nodes</em>: split the blockchain functionality in components that together make up the full blockchain.</li>
<li><em>Shard the blockchain</em>: split the chain itself into smaller pieces that communicate through a single coordination chain</li>
</ul>
<p>All of these solutions have their disadvantages and none of the solutions solve the basic problem.</p>
<h2 id="blockchain-sharding"><a class="header" href="#blockchain-sharding">blockchain sharding</a></h2>
<p>The practice of splitting a blockchain into multiple segments is known as sharding. This feature is planned for Ethereum 2.0 and is already live on the Zilliqa network. Sharding is presented as an emerging solution to the blockchain trilemma.</p>
<table><thead><tr><th style="text-align: left"><em>Strengths</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Reduces both the storage and computational load of nodes securing each shard, thus improving scalability and decentralization. Could provide better security than multichain ecosystems which appear to have a similar structure.</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Weaknesses</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Sharding is still an experimental technology with limited testing. Adding sharding to an existing blockchain requires significant changes to the protocol itself.</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Opportunities</em></th></tr></thead><tbody>
<tr><td style="text-align: left">If successful, sharding can be a powerful feature and may find wide adoption among blockchains. Node operators on these chains still stand to benefit from distributed archiving of their shard.</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Threats</em></th></tr></thead><tbody>
<tr><td style="text-align: left">This is a complex addition to a protocol that comes with certain tradeoffs and adds additional attack surface.</td></tr>
</tbody></table>
<h2 id="lower-the-number-of-full-nodes-in-a-blockchain"><a class="header" href="#lower-the-number-of-full-nodes-in-a-blockchain">Lower the number of full nodes in a blockchain</a></h2>
<p>Full nodes refer to blockchain nodes that contain and have 100% of the chain history available online with a complete history intact, true and secure for the whole chain.  In order to control the quantity and quality of the nodes that make up the chain some project restrict participation to only known entities. This is often referred to as a &quot;permissioned&quot; chain.  A few <em>permissioned</em> organizations (or people) are tasked to provide full nodes and deal with all the operational, security, scaling and reliability tasks.  This can be with and without a rewards for doing so.</p>
<p>Non-permissioned nodes can be allowed to alleviate some of the operational workload, but these nodes are not considered to be authoritative for the true chain history. The true chain history is only provided by the (few) permissioned (full) chain nodes. This solution is chosen by protocol implementations to circumvent the problem, by having node operators that commit to put the investment in to have proper node sizing. </p>
<p><strong>SWOT analysis</strong></p>
<table><thead><tr><th style="text-align: left"><em>Strengths</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Efficient way of guaranteeing correct, secure and scalable chain operations</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Weaknesses</em></th></tr></thead><tbody>
<tr><td style="text-align: left">It steps away from the decentralization benefit a blockchain presents over traditional database solutions. The strength of a blockchain solution is that anyone can opt in to contribute and help to achieve security and safety. By making blockchains permissioned the security and safety is lowered because the full nodes are operated by a set number of organizations and people</td></tr>
<tr><td style="text-align: left">One node still has the full volume required, which makes running it and managing the issues around very big volumes still relevant.</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Opportunities</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Create a governance structure around blockchain technology that is in line with the traditional IT industry and paradigms while still bringing forward the innovation achieved by blockchain technology.</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Threats</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Secure  operations is threated by having a limited, known number of permissioned operators. The form a set of single points of failure for the whole chains operations and security.</td></tr>
</tbody></table>
<h2 id="compress-data"><a class="header" href="#compress-data">Compress data</a></h2>
<p>This is probably the most used method to solve the issue. All participating archival nodes compress the blocks created to minimize the data storage footprint. There are many compression algorithms available and depending on the specific type of blockchain hardware certain algorithms present better options than others.</p>
<p><strong>SWOT analysis</strong></p>
<table><thead><tr><th style="text-align: left"><em>Strengths</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Simple to implement, plenty of compression algorithms available</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Weaknesses</em></th></tr></thead><tbody>
<tr><td style="text-align: left">it does not solve the issue, it postpones the issue becoming a problem.</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Opportunities</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Quick to implement, quick to deploy.</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Threats</em></th></tr></thead><tbody>
<tr><td style="text-align: left">The storage problem will sooner or later appear. Choosing this solution will allow you to go to market quickly but then you have to hope that a solution will present itself before the issue will become a problem.</td></tr>
</tbody></table>
<h2 id="a-hierarchy-of-nodes"><a class="header" href="#a-hierarchy-of-nodes">A hierarchy of nodes</a></h2>
<p>Another way to overcome this issue is to have a blockchain split the full chain process (workload) into subprocesses. Each subprocess can now be worked on by specific type of nodes that are tailored to operate that one part of the process.  Examples are:</p>
<ul>
<li>Transaction nodes: a node that only provides transaction capabilities and possibly store the results</li>
<li>Validator nodes: nodes that look at the transaction nodes and validates transaction through some sort of consensus mechanism</li>
<li>Core/full nodes: nodes that store the full chain history and are authoritative for chain content </li>
<li>Edge nodes: remote nodes that perform edge transaction in a fast and efficient way.  There has to be some form of synchronization between edge nodes and core/full nodes.</li>
</ul>
<p>By creating different nodes type the requirements for storage per node (type) are lowered.</p>
<p><strong>SWOT analysis</strong> </p>
<table><thead><tr><th style="text-align: left"><em>Strengths</em></th></tr></thead><tbody>
<tr><td style="text-align: left">provides independent scalability between different functions / processes off the full chain.</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Weaknesses</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Overhead is created by relying on the network to communicate between the different functions/subprocesses. Network speed might effect overall performance on the chain</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Opportunities</em></th></tr></thead><tbody>
<tr><td style="text-align: left">when required subprocess's can easily be broken up into smaller pieces when needed or vice versa integrated into large components.</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Threats</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Many different components have network interfaces and there are is a realistic chance that hackers might attack the network interfaces from the different nodes.</td></tr>
</tbody></table>
<h2 id="a-new-way-how-to-store-data-using-forward-looking-correcting-codes"><a class="header" href="#a-new-way-how-to-store-data-using-forward-looking-correcting-codes">A new way how to store data using forward looking correcting codes</a></h2>
<p>This is what ThreeFold has been working on for a long time, the original solution was created by our team back in 2012 and was part of an exit of 300m USD. The software we have today is a completely new version and is even faster and more scalable.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="casper-uses-lmdb"><a class="header" href="#casper-uses-lmdb">Casper uses LMDB</a></h1>
<p>CasperLabs uses <a href="https://medium.com/casperlabs/casperlabs-releases-node-0-3-c3a1adb2d645">LMDB</a> which provides some features we can tap into to provide pruning support.</p>
<p>LMDB is a big file on a filesystem and as such its hard to use some storage tricks on the quantum safe filesystem to implement pruning on that level.</p>
<p>To succeed with pruning on casperlabs we have to integrate in the storage layer itself, probably in the code of casperlabs itself which complicates this project.</p>
<h3 id="lightning-memory-mapped-database-manager-lmdb"><a class="header" href="#lightning-memory-mapped-database-manager-lmdb">Lightning Memory-Mapped Database Manager (LMDB)</a></h3>
<p><em>LMDB is a Btree-based database management library modeled loosely on the BerkeleyDB API, but much simplified. The entire database is exposed in a memory map, and all data fetches return data directly from the mapped memory, so no malloc's or memcpy's occur during data fetches. As such, the library is extremely simple because it requires no page caching layer of its own, and it is extremely high performance and memory-efficient. It is also fully transactional with full ACID semantics, and when the memory map is read-only, the database integrity cannot be corrupted by stray pointer writes from application code.</em></p>
<p><em>The library is fully thread-aware and supports concurrent read/write access from multiple processes and threads. Data pages use a copy-on- write strategy so no active data pages are ever overwritten, which also provides resistance to corruption and eliminates the need of any special recovery procedures after a system crash. Writes are fully serialized; only one write transaction may be active at a time, which guarantees that writers can never deadlock. The database structure is multi-versioned so readers run with no locks; writers cannot block readers, and readers don't block writers.</em></p>
<p><em>Unlike other well-known database mechanisms which use either write-ahead transaction logs or append-only data writes, LMDB requires no maintenance during operation. Both write-ahead loggers and append-only databases require periodic checkpointing and/or compaction of their log or database files otherwise they grow without bound. LMDB tracks free pages within the database and re-uses them for new write operations, so the database size does not grow without bound in normal use.</em></p>
<p><em>The memory map can be used as a read-only or read-write map. It is read-only by default as this provides total immunity to corruption. Using read-write mode offers much higher write performance, but adds the possibility for stray application writes thru pointers to silently corrupt the database. Of course if your application code is known to be bug-free (...) then this is not an issue.</em></p>
<p>See on internet: <a href="http://www.lmdb.tech/doc/index.html">LMDB</a></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="mermaid.min.js"></script>
        <script type="text/javascript" src="mermaid-init.js"></script>

        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </body>
</html>
