<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>DEVXDAO PRUNING GRANT 37</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="grant.html">Grant</a></li><li class="chapter-item expanded "><a href="research/research.html"><strong aria-hidden="true">1.</strong> Research</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="industry/the_problem.html"><strong aria-hidden="true">1.1.</strong> The Problem</a></li><li class="chapter-item expanded "><a href="industry/analysis.html"><strong aria-hidden="true">1.2.</strong> Analysis</a></li><li class="chapter-item expanded "><a href="research/storage_approaches/storage_approaches_include.html"><strong aria-hidden="true">1.3.</strong> Storage Approaches</a></li><li class="chapter-item expanded "><a href="research/thin_provisioning.html"><strong aria-hidden="true">1.4.</strong> Thin Provisioning</a></li><li class="chapter-item expanded "><a href="research/pokt_network.html"><strong aria-hidden="true">1.5.</strong> Pokt.Network Achievement</a></li><li class="chapter-item expanded "><a href="research/casper_data_structure.html"><strong aria-hidden="true">1.6.</strong> CasperLabs Datastructure</a></li><li class="chapter-item expanded "><a href="research/storage_integration_qsfs.html"><strong aria-hidden="true">1.7.</strong> CasperLabs Integration</a></li><li class="chapter-item expanded "><a href="research/solution_ideas.html"><strong aria-hidden="true">1.8.</strong> Solution Ideas</a></li></ol></li><li class="chapter-item expanded "><a href="solution/solution.html"><strong aria-hidden="true">2.</strong> Solution</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="solution/architecture.html"><strong aria-hidden="true">2.1.</strong> Architecture</a></li><li class="chapter-item expanded "><a href="solution/qsss_benefits.html"><strong aria-hidden="true">2.2.</strong> Storage System Benefits</a></li></ol></li><li class="chapter-item expanded "><a href="requirements/requirements_intro.html"><strong aria-hidden="true">3.</strong> Requirements</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="requirements/requirements_high_level.html"><strong aria-hidden="true">3.1.</strong> Requirements Overview</a></li><li class="chapter-item expanded "><a href="requirements/necessary_conditions.html"><strong aria-hidden="true">3.2.</strong> Necessary Conditions</a></li><li class="chapter-item expanded "><a href="requirements/solution_vs_necessary_conditions.html"><strong aria-hidden="true">3.3.</strong> Solutions vs Requirements</a></li></ol></li><li class="chapter-item expanded "><a href="technology/technology.html"><strong aria-hidden="true">4.</strong> Technology</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="technology/qsss/qsss_home.html"><strong aria-hidden="true">4.1.</strong> Quantum Safe Storage</a></li><li class="chapter-item expanded "><a href="technology/qsss/qss_algorithm.html"><strong aria-hidden="true">4.2.</strong> Quantum Safe Algo</a></li><li class="chapter-item expanded "><a href="technology/primitives/compute/compute.html"><strong aria-hidden="true">4.3.</strong> Compute</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">DEVXDAO PRUNING GRANT 37</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <p><img src="img/casper_devxdao.png" alt="" /></p>
<h1 id="grant"><a class="header" href="#grant">Grant</a></h1>
<ul>
<li>Received Grant Before: No</li>
<li>Grant amount total; 500k</li>
<li>Grant Id:37</li>
<li>Completion: 80%</li>
<li>User Id: 168 (maybe this should be relocated)</li>
<li>Company or Organization: ThreeFold</li>
</ul>
<h2 id="benefits"><a class="header" href="#benefits">Benefits</a></h2>
<p>If successful the project will bring new scaling opportunity for level-1 protocols to execute value transactions and smart contracts. </p>
<p>Problem: the current technology reality is that level-1 blockchains carry forever it's history which will put an increasing demand on storage requirements for all participating nodes. This is not very sustainable or efficients.</p>
<p>The number of implementations of DAO's is goinf to be exponentially growing and there is a need for level-1 blockchain protocols to be able to deal with the increased volume of transactions and history where pruning is going to be needed.</p>
<p>Our Solution is</p>
<ul>
<li>Thin provisioning can deploy blockchains with at least 3x storage requirements and bandwidth requirements for intial deployment (done)
<ul>
<li>Thin provisioning works for most blockchain systems, this has now been proven. (done)</li>
<li>Further integration storage backend of casperlabs would benefit even more. (next grant)</li>
</ul>
</li>
<li>Pruning support for blockchains 
<ul>
<li>Show how a solution can be created using ThreeFold storage components (is using 30 years of experience)</li>
<li>Demonstrate applicability</li>
<li>Create specs &amp; solution upto to a pruning DB which can then be used in a next grant to integrate tightly in casperlabs DB</li>
</ul>
</li>
</ul>
<blockquote>
<p>The result of this grant are massive important for the blockchain industry, storage is a big issue.</p>
</blockquote>
<h2 id="our-team-has-massive-experience-in-storage--internet-technology"><a class="header" href="#our-team-has-massive-experience-in-storage--internet-technology">Our Team has massive experience in storage &amp; internet technology</a></h2>
<ul>
<li>first data deduplication system in the world for datacenters &amp; backup (acquired by veritas/symantec) (2002)</li>
<li>first always consistent distributed key value database in the world https://dbdb.io/db/arakoon (2010-2015)</li>
<li>first proof of blockstake blockchains in the world (was called Rivine, discontinued because of lack of funds)</li>
<li>first distributed storage system based on forward looking correcting codes (see Amplidata, used by major governments), scales to zetabytes</li>
<li>probably first and super high performance always consistent disk volume driver on top of Arakoon and Amplidata, we were able to demonstrate 10.000.000 IOPS in 1 rack back in 2013</li>
<li>first distributed block based storage system on top of Infiniband for ZFS (was done just before we were acquired by SUN microsystems)</li>
<li>major world records in relation to hosting of very big internet sites (1998-2000) with global loadbalancing and caching at a time this was not done yet</li>
</ul>
<blockquote>
<p>We are delighted to use this expertise for the benefit of the world and are grateful for the chance devxdao gives us to develop and contribute even more.</p>
</blockquote>
<h2 id="milestone-1-100-done--250k-usd"><a class="header" href="#milestone-1-100-done--250k-usd">Milestone 1 (100% done) = 250k USD</a></h2>
<ul>
<li>Milestone title: Initial pruning research document</li>
<li>The portion that the OP is requesting from the total grant for the milestone: 
<ul>
<li>USD 250.000 (was $100k originally)</li>
</ul>
</li>
<li>Due date: 
<ul>
<li>done</li>
</ul>
</li>
<li>Details of what will be delivered:
<ul>
<li>prove Layer-1 blockchain thin provisioning can be done (this is a massive achievent and has incredible benefits for the industry)</li>
<li>prove Layer-1 blockchain pruning can be done, come up with required architecture</li>
<li>solution for thin provisioning based on threefold quantum safe filesystem (this requires changes in this system)</li>
<li>Initial research will be done with the Casper Labs Layer-1 protocol and also some other blockchains.</li>
<li>Create the solution which allows everyone to verify thin provisioning can be done.</li>
</ul>
</li>
</ul>
<blockquote>
<p>remarks: we were an early grant and were trying to get feedback before submitting because there was no clarity for us how to do it and how sure we would be on the grant. We might have been not good enough in our communication, but the grant got removed before we could receive funding for the work done. We hope that this might be recovered, as such we have updated this document to reflect our current situation and work done.</p>
</blockquote>
<h3 id="acceptance-criteria"><a class="header" href="#acceptance-criteria">Acceptance Criteria:</a></h3>
<blockquote>
<p>we hope that the reader can see how this work has an incredible potential for the blockchain industry. We have been able to show how blockchains can be deployed using more than 5x time and bandwith benefit, this shows the potential of how pruning &amp; thin provisioning migh be achieved.</p>
</blockquote>
<ul>
<li><input disabled="" type="checkbox" checked=""/>
thin provisioning solution can be deployed (based on pokt.network for now) and can be done by everyone</li>
<li><input disabled="" type="checkbox" checked=""/>
see how the thin provisioning cuts heavily on bandwidth and storage requirements (5x benefits easy to achieve)</li>
<li><input disabled="" type="checkbox" checked=""/>
deploy a large blockchain in less than 1h which would have taken days before</li>
<li><input disabled="" type="checkbox" checked=""/>
all required code to allow everyone to experiment (see pokt.network in research)</li>
<li><input disabled="" type="checkbox" checked=""/>
all code is opensource (threefold as well as the deployers for thin provisioning)</li>
<li><input disabled="" type="checkbox" checked=""/>
research for industry, different approaches around thin provisioning &amp; pruning (see research part of document)</li>
<li><input disabled="" type="checkbox" checked=""/>
define solution which allows implementation for pruning for casper labs</li>
<li><input disabled="" type="checkbox" checked=""/>
do detailed research about casper labs to see how integration can be done (and if): our conclusion it can be done</li>
<li><input disabled="" type="checkbox" checked=""/>
link to required technology parts as provided by ThreeFold, see if we can use those solutions and what benefit would be</li>
<li><input disabled="" type="checkbox" checked=""/>
do all required work to let it work on ThreeFold Grid</li>
</ul>
<h2 id="milestone-2-90-done--150k-usd"><a class="header" href="#milestone-2-90-done--150k-usd">Milestone 2 (90% done) = 150k USD</a></h2>
<p>implement all required components for base system as defined in <a href="../solution/solution.html">solution part of doc</a></p>
<h3 id="acceptance-criteria-1"><a class="header" href="#acceptance-criteria-1">Acceptance Criteria:</a></h3>
<p>We have been able to leverage a lot of technology from Threefold Tech which has been developed over the last 10 years. This grant was all about making sure and improving this codebase so it can be used for casperlabs. This is the result of hundreds of manmonths for which we are delighted to receive some recognition in a form of a grant.</p>
<ul>
<li><input disabled="" type="checkbox" checked=""/>
zdb with all required features as required for the prune DB (opensource)</li>
<li><input disabled="" type="checkbox" checked=""/>
zdb storage containers</li>
<li><input disabled="" type="checkbox" checked=""/>
caching capability if the technology</li>
<li><input disabled="" type="checkbox" checked=""/>
FLECC codec implemented and available as golang code (opensource)</li>
<li><input disabled="" type="checkbox" checked=""/>
ZDB storage containers caching &amp; encoding with FLECC codec.</li>
<li><input disabled="" type="checkbox" checked=""/>
FLEXX encoding &amp; distribution to 20+ backends</li>
<li><input disabled="" type="checkbox" checked=""/>
self healing on the data, prove that data cannot be corrupted</li>
<li><input disabled="" type="checkbox" checked=""/>
automatic recovery of broken ZDB instances on nodes</li>
<li><input disabled="" type="checkbox" checked=""/>
deployment system on top of TFGrid </li>
<li><input disabled="" type="checkbox" checked=""/>
metadata system</li>
<li><input disabled="" type="checkbox" checked=""/>
read path is redundant, can come from multiple ZDB backends</li>
<li><input disabled="" type="checkbox"/>
better description of the solution</li>
<li><input disabled="" type="checkbox"/>
documentation and make demonstration tutorials in text and video</li>
<li><input disabled="" type="checkbox"/>
document all links to code &amp; scripts as used on above</li>
</ul>
<h2 id="milestone-3-0-done--100k-usdprunedb--testing--documentation"><a class="header" href="#milestone-3-0-done--100k-usdprunedb--testing--documentation">Milestone 3 (0% done) = 100k USD:prunedb + testing &amp; documentation</a></h2>
<ul>
<li><input disabled="" type="checkbox"/>
prune db</li>
<li><input disabled="" type="checkbox"/>
examples in VLANG or GOLANG how to use the prunedb for a blockchain like casperlabs</li>
<li><input disabled="" type="checkbox"/>
performance test</li>
<li><input disabled="" type="checkbox"/>
scalability test</li>
<li><input disabled="" type="checkbox"/>
reliability test</li>
<li><input disabled="" type="checkbox"/>
documentation</li>
</ul>
<h2 id="future-grants"><a class="header" href="#future-grants">Future Grants</a></h2>
<h3 id="integration-in-casperlabs-blockchain-backend"><a class="header" href="#integration-in-casperlabs-blockchain-backend">Integration in Casperlabs Blockchain Backend</a></h3>
<ul>
<li>show how seamless pruning can be implemented in POC using the blockchain engine</li>
<li>this is a serious job and will require quite some effort to make sure it happens in the safest way and gets well tested.</li>
<li>this will require low level integration so that the data paths dont change to much.</li>
<li>Use the Blockchain layer to also prove authenticity of the pruning storage layer (is an extra check, not strictly needed, but might give extra confidence to community).</li>
</ul>
<h3 id="redundant-write-path-for-pruning-db"><a class="header" href="#redundant-write-path-for-pruning-db">Redundant write path for pruning DB</a></h3>
<p>Use e.g. tendermint to write in redundant way to the backend.</p>
<h3 id="production-readyness--testing"><a class="header" href="#production-readyness--testing">Production Readyness &amp; Testing</a></h3>
<p>Testing this new pruning layer in all its facets on performance and reliability level.</p>
<h3 id="master-slave-solution-for-pruning-db"><a class="header" href="#master-slave-solution-for-pruning-db">Master-Slave Solution for pruning DB</a></h3>
<ul>
<li>Make Pruning DB Redundant (active-active or active-passive)</li>
<li>Show how there is never a reliability issue, create good test cases to show this redundancy.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="research-intro"><a class="header" href="#research-intro">Research Intro</a></h1>
<p>This research document represents the first milestone in the grant that has been awarded by DEVxDAO. This document will lay out the challenges that need to be overcome to create a lasting generic solution for the blockchain forever growing data storage problem and then identifies solution paths to this challenge. This document will present a number of solution paths to be looked at and then, as a conclusion, a proof of the solution will be implemented in milestone 2 of this grant.</p>
<p>Extensive research has been done by a group called ThreeFold into creating a decentralized technology with compute and storage capabilities to build a new internet that is not in the hands of a few monopolies. This research is centered around a solid &quot;layer-0&quot; software stack and we intend to put it to work in conjunction with new innovation to find a decentralized autonomous solution to the blockchain pruning problem.</p>
<p>This research document will look at publications and literature to see what history has taught us how to deal with growing datasets and increasing importance of digital data. Areas that will be investigated are the well known area of backups (full and incremental) and archiving. Another area which might get us some insights is de-duplication of data before it is archived or backed up. Also, we will look at other publications describing research or projects about finding a (partial) solution to the pruning challenge.</p>
<p>The biggest work is to test the algorithms available to ThreeFold for a blockchain like Casperlabs and see if benefits can be achieved.</p>
<p>After reviewing what has been developed, implemented and proposed as solutions, we will present a number of methods by which we believe we can get to an acceptable pruning solution. We will present SWOT analyses on the methods presented and select the few that are most feasible to get to the required result.</p>
<p>Pruning is not the only solution to the data problem, thin provisioning is as important. Thin provisioning means that not every blockchain needs to sync all available information. </p>
<p>The selected methods will then be architected to a level where we can see how this would work in a real life situation and what is required to achieve this. Recommendations for the second milestone will be presented and risks and limitations will be cited.</p>
<h2 id="purpose-thin-provisioning--pruning-research"><a class="header" href="#purpose-thin-provisioning--pruning-research">Purpose: thin provisioning &amp; pruning research</a></h2>
<p>Layer-1 blockchain pruning is key to its growth and applicability to any blockchain and/or DAO growth. Initial research will be done with the CasperLabs layer-1 protocol.
If successful, the project will bring new scaling opportunities for level-1 protocols to continue to execute value transactions and smart contracts without performance loss for ever. The current technological reality is that level-1 blockchains will carry forever their history, which will put an increasing demand on storage requirements for participating (full) nodes retaining a complete copy of the chain. This is not very sustainable and efficient. Furthermore, it effectively leads to centralization by limiting the number of network participants which have the resources to deploy adequate storage (and to a lesser extend compute) capacity to run a full node.</p>
<p>Possible solutions may defer the problem into the future (compression of data) or require complex structural changes to the protocol itself (different node sub-processes/roles or sharding). The question we propose to address is:  &quot;How can nodes participate in consensus and the maintenance of a full blockchain without needing to store a complete redundant copy of the chain data locally?&quot;.</p>
<p>Many protocols already offer &quot;pruning&quot; features within their node software, so that while the full chain is downloaded and verified, only the newest blocks are retained. These nodes are able to participate in consensus and provide a secure way to interact with the network, but they offer limited benefit in the securing and maintaining of the chain.</p>
<p>Implementations and deployments of decentralized systems like DAOs rely on blockchains for smart contract and transaction storage are expected to grow exponentially. In order for these systems to remain secure and truly decentralized, a solution is needed.</p>
<!-- 
- [casper_deployment](@casper_deployment)
- [qsfs_performance](@qsfs_performance)
- [storage_integration](@storage_integration) --><div style="break-before: page; page-break-before: always;"></div><h1 id="the-problem-forever-growing-data"><a class="header" href="#the-problem-forever-growing-data">The problem: Forever growing data</a></h1>
<p>Definition from the <a href="https://en.wikipedia.org/wiki/Blockchain">wiki</a>:</p>
<blockquote>
<p>A blockchain is a growing list of records, called blocks, that are linked together using cryptography. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a Merkle tree). The timestamp proves that the transaction data existed when the block was published in order to get into its hash. As blocks each contain information about the block previous to it, they form a chain, with each additional block reinforcing the ones before it. Therefore, blockchains are resistant to modification of their data because once recorded, the data in any given block cannot be altered retroactively without altering all subsequent blocks.</p>
</blockquote>
<p>Therefore blockchains by design create and carry historical data forever. The nature of a blockchain is that it stores data immutably and in an always-append manner. So from the moment of inception of a blockchain (the &quot;genesis&quot; block) to the current point in time (the latest block) all data (blocks) ever created need to be stored on <em>and</em> available on a sufficient set of (full) nodes to ensure the life of the chain.</p>
<p>This has profound implications for successful blockchains, regardless of their purpose. Whether the blockchain is a pure value transaction blockchain or it is a smart contract system with additional data storage requirements, it is all compounding to a forever growing storage requirement. To the extent that the data grows faster than the storage purchasing power of node operators, this becomes a centralizing force as fewer network participants can afford to host the whole chain.</p>
<p>There are two main categories of blockchains, each with specific benefits and disadvantages: permission<strong>less</strong> and permission<strong>ed</strong> blockchains. </p>
<p>The first allows anyone to take part and start contributing to the network, the latter having a (central) authority that agrees who can participate and who not. </p>
<p>Both types of blockchains have the same challenge with regards to data storage: it is a forever growing chain of blocks which increases the storage needs of participating nodes indefinitely, but pruning solutions are very different because of the following:</p>
<ul>
<li>in a permission<strong>less</strong> blockchain anyone can choose to participate to help to run and secure the blockchain by operating a blockchain node.  Consensus and trust is based on an algorithm, not on any central authority.</li>
<li>in a permission<strong>ed</strong> blockchain there is a central authority of some sort that allows blockchain nodes to be added to the network.  The central authority provides trust and to a degree consensus.</li>
</ul>
<p>So the blockchain pruning challenge is many times larger for permission<strong>less</strong> chains then for permission<strong>ed</strong> chains.</p>
<h2 id="evidence-of-the-problem-statement"><a class="header" href="#evidence-of-the-problem-statement">Evidence of the problem statement</a></h2>
<p>Overview (not exhaustive) of what the current blockchains experience in terms of data growth and the challenges that come with it at the current state of play of blockchain projects:</p>
<ul>
<li><a href="https://developers.stellar.org/docs/run-core-node/prerequisites/">Stellar</a></li>
<li><a href="https://decrypt.co/24779/ethereum-archive-nodes-now-take-up-4-terabytes-of-space">Ethereum</a></li>
<li><a href="https://101blockchains.com/blockchain-size/">blockchain size article</a></li>
</ul>
<p>Reading through these articles we can find chain size growth figures of 50x over a period of 2 years. And with the exponential growth and adoption of blockchain technologies this is a real problem that lies ahead of us which is not going to be solved by the rate of which hardware is improving and becoming more affordable.</p>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>The blockchain industry needs a way how to prune old data, which means old data needs to be archived in such a way that the system still keeps on working.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="analysis"><a class="header" href="#analysis">Analysis</a></h1>
<p>The current solutions to the forever growing data storage requirements to not tackle the real problem: How can we prune a  blockchain protocol without putting limitations and restrictions on the protocol itself but provide a storage utility that is trustworthy and builds on the principles provided by the blockchain protocol without changing them?</p>
<h2 id="current-solutions-to-forever-growing-data-in-blockchains"><a class="header" href="#current-solutions-to-forever-growing-data-in-blockchains">Current solutions to forever growing data in blockchains</a></h2>
<p>A non-exhaustive list of ways to deal with the data growth issue is:</p>
<ul>
<li><em>Compress data</em>: keep storing data as is but lower the impact / footprint of it. Most already do this.</li>
<li><em>Lower the number of full nodes in a blockchain</em>: make the number of nodes smaller in order to have less copies of the data distributed which lowers the synchronization requirements (to retain a high transaction speed) and limit the overall hardware needed to operate the full nodes.  This can be done by staking mechanisms and creating a set number of full nodes available in a chain.</li>
<li><em>Create a hierarchy of nodes</em>: split the blockchain functionality in components that together make up the full blockchain.</li>
<li><em>Shard the blockchain</em>: split the chain itself into smaller pieces that communicate through a single coordination chain</li>
<li><em>Introduce a new storage paradigm</em>: there are new codecs which might help to resolve the problem.</li>
</ul>
<blockquote>
<p>It doesn't have to be 1 solution but migh be a combination of above.</p>
</blockquote>
<h2 id="blockchain-sharding"><a class="header" href="#blockchain-sharding">blockchain sharding</a></h2>
<p>The practice of splitting a blockchain into multiple segments is known as sharding. This feature is planned for Ethereum 2.0 and is already live on the Zilliqa network. Sharding is presented as an emerging solution to the blockchain trilemma.</p>
<table><thead><tr><th style="text-align: left"><em>Strengths</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Reduces both the storage and computational load of nodes securing each shard, thus improving scalability and decentralization.</td></tr>
</tbody></table>
<BR>
<table><thead><tr><th style="text-align: left"><em>Weaknesses</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Sharding is still an experimental technology and has issues in relation to always consistancy.</td></tr>
<tr><td style="text-align: left">Adding sharding to an existing blockchain requires significant changes to the protocol itself. Its a very hard approach to get right, keeping a global state some say is impossible.</td></tr>
</tbody></table>
<BR>
<table><thead><tr><th style="text-align: left"><em>Opportunities</em></th></tr></thead><tbody>
<tr><td style="text-align: left">If successful, sharding can be a powerful feature and may find wide adoption among blockchains.</td></tr>
<tr><td style="text-align: left">Node operators on these chains still stand to benefit from distributed archiving of their shard.</td></tr>
</tbody></table>
<BR>
<table><thead><tr><th style="text-align: left"><em>Threats</em></th></tr></thead><tbody>
<tr><td style="text-align: left">This is a complex addition to a protocol that comes with certain tradeoffs and adds additional attack surface.</td></tr>
</tbody></table>
<h2 id="lower-the-number-of-full-nodes-in-a-blockchain"><a class="header" href="#lower-the-number-of-full-nodes-in-a-blockchain">Lower the number of full nodes in a blockchain</a></h2>
<p>Full nodes refer to blockchain nodes that contain and have 100% of the chain history available online with a complete history intact, true and secure for the whole chain.  In order to control the quantity and quality of the nodes that make up the chain some project restrict participation to only known entities. This is often referred to as a &quot;permissioned&quot; chain.  A few <em>permissioned</em> organizations (or people) are tasked to provide full nodes and deal with all the operational, security, scaling and reliability tasks.  This can be with and without a rewards for doing so.</p>
<p>Non-permissioned nodes can be allowed to alleviate some of the operational workload, but these nodes are not considered to be authoritative for the true chain history. The true chain history is only provided by the (few) permissioned (full) chain nodes. This solution is chosen by protocol implementations to circumvent the problem, by having node operators that commit to put the investment in to have proper node sizing. </p>
<p><strong>SWOT analysis</strong></p>
<table><thead><tr><th style="text-align: left"><em>Strengths</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Efficient way of guaranteeing correct, secure and scalable chain operations. This can be a costeffective way how to achieve a solution.</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Weaknesses</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Its less decentralized.</td></tr>
<tr><td style="text-align: left">The strength of a blockchain solution is that anyone can opt in to contribute and help to achieve security and safety.</td></tr>
<tr><td style="text-align: left">By making blockchains permissioned the security and safety is lowered because the full nodes are operated by a set number of organizations and people</td></tr>
<tr><td style="text-align: left">One node still has the full volume required, which makes running it and managing the issues around very big volumes still relevant.</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Opportunities</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Create a governance structure around blockchain technology that is in line with the traditional IT industry</td></tr>
<tr><td style="text-align: left">Easier to get SLA's in place</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Threats</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Secure  operations is threatened by having a limited, known number of permissioned operators.</td></tr>
<tr><td style="text-align: left">The form a set of single points of failure for the whole chains operations and security.</td></tr>
</tbody></table>
<h2 id="compress-data"><a class="header" href="#compress-data">Compress data</a></h2>
<p>Not really a solution, rather something everyone should do one way or the other and many are already doing. All participating archival nodes compress the blocks created to minimize the data storage footprint. There are many compression algorithms available and depending on the specific type of blockchain hardware certain algorithms present better options than others.</p>
<p><strong>SWOT analysis</strong></p>
<table><thead><tr><th style="text-align: left"><em>Strengths</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Simple to implement, plenty of compression algorithms available. Everyone should implement this system.</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Opportunities</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Quick to implement, quick to deploy. Its not a real solution, it just helps to postpone the problem somewhat, but most already have it build in</td></tr>
</tbody></table>
<h2 id="a-hierarchy-of-nodes"><a class="header" href="#a-hierarchy-of-nodes">A hierarchy of nodes</a></h2>
<p>Another way to overcome this issue is to have a blockchain split the full chain process (workload) into subprocesses. Each subprocess can now be worked on by specific type of nodes that are tailored to operate that one part of the process.  Examples are:</p>
<ul>
<li>Transaction nodes: a node that only provides transaction capabilities and possibly store the results</li>
<li>Validator nodes: nodes that look at the transaction nodes and validates transaction through some sort of consensus mechanism</li>
<li>Core/full nodes: nodes that store the full chain history and are authoritative for chain content </li>
<li>Edge nodes: remote nodes that perform edge transaction in a fast and efficient way.  There has to be some form of synchronization between edge nodes and core/full nodes.</li>
</ul>
<p>By creating different nodes type the requirements for storage per node (type) are lowered.</p>
<p><strong>SWOT analysis</strong> </p>
<table><thead><tr><th style="text-align: left"><em>Strengths</em></th></tr></thead><tbody>
<tr><td style="text-align: left">provides independent scalability between different functions / processes off the full chain.</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Weaknesses</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Overhead is created by relying on the network to communicate between the different functions/subprocesses. Network speed might effect overall performance on the chain</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Opportunities</em></th></tr></thead><tbody>
<tr><td style="text-align: left">when required subprocess's can easily be broken up into smaller pieces when needed or vice versa integrated into large components.</td></tr>
</tbody></table>
<table><thead><tr><th style="text-align: left"><em>Threats</em></th></tr></thead><tbody>
<tr><td style="text-align: left">Complexity might lead to less security, simplicity is always the friend of security.</td></tr>
</tbody></table>
<h2 id="a-new-way-how-to-store-data-using-forward-looking-correcting-codes"><a class="header" href="#a-new-way-how-to-store-data-using-forward-looking-correcting-codes">A new way how to store data using forward looking correcting codes</a></h2>
<p>This is what ThreeFold has been working on for a long time, the original solution was created by our team back in 2012 and was part of an exit of 300m USD. The software we have today is a completely new version and is even faster and more scalable.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="storage-literature-research-and-remarks"><a class="header" href="#storage-literature-research-and-remarks">Storage Literature Research and Remarks</a></h1>
<p>Here some additional sources where information can be found on efforts on addessing blockchain storage concerns. </p>
<p>This section does not try to provide an exhaustive overview its just inspiration to come to the possible methodologies to find a generic solution.</p>
<h2 id="chain-state-snapshots"><a class="header" href="#chain-state-snapshots">Chain state snapshots</a></h2>
<p>In addition to archiving the chain itself, verifiable archives of the ledger could provide an expedited way to restore node functionality or bootstrap new nodes.</p>
<p>Work has been done within the Bitcoin community around enabling backups of the chain state database. </p>
<p>See this <a href="https://github.com/bitcoin/bitcoin/issues/8037">discussion</a> on considerations for such an approach and <a href="https://github.com/bitcoin/bitcoin/pull/15606">code</a> produced to implement it.</p>
<h3 id="erasure-encoding-for-blockchains"><a class="header" href="#erasure-encoding-for-blockchains">Erasure encoding for blockchains</a></h3>
<p>Erasure coding means a mechanism that data gets cut into pieces so that less overhead is required to store information over multiple disks.</p>
<p>Erasure coding is a simple form of &quot;forward looking error correcting codes&quot;</p>
<p>The question of applying erasure encoding to blockchain data in order to reduce the storage requirements of individual nodes has been explored in this <a href="https://arxiv.org/pdf/1805.00860.pdf">paper</a>. </p>
<p>The authors conclude that such an approach can provide strong assurance of data correctness without large computational overhead.</p>
<p>We believe this is a very elegant way and should be done but care needs to be taken for</p>
<ul>
<li>if using erasure coding technologies like reed solomon, there is a possiblity to loose data (also called the raid5 hole)</li>
<li>order of operation is still important in reed solomon</li>
<li>there is not that much flexibility in data positioning and systems can be rather fragile if not done properly especially in networked environments</li>
</ul>
<p>We as threefold are a big proponent of this approach if done properly.</p>
<h3 id="proofs-of-retrievability"><a class="header" href="#proofs-of-retrievability">Proofs of Retrievability</a></h3>
<p>Nodes who agree to share the task of storing blockchain data have an interest in verifying that the data is in fact recoverable. </p>
<p>To avoid the overhead associated with actually recovering the data, a compact computational proof may be used instead.</p>
<p><a href="http://www.arijuels.com/wp-content/uploads/2013/09/BJO09b.pdf">These authors</a> describe an algorithm based on erasure encoding and random sampling to generate such proofs.</p>
<p>There are multiple ways how proof of retrievability can be implemented as ThreeFold we have implemented a novel way how to do this without the overhead of what is described above.</p>
<h3 id="stateless-clients-sharding-and-scalability"><a class="header" href="#stateless-clients-sharding-and-scalability">Stateless clients, sharding, and scalability</a></h3>
<p>Ethereum's Vitalik Buterin has written extensively on the problem of blockchain scalability and the importance of keeping node operation accessible for the health of blockchain networks. He <a href="https://ethresear.ch/t/the-stateless-client-concept/172">proposes</a> that beyond pruning the chain, nodes might not need to store any state at all to validate blocks and help secure the network.</p>
<p><a href="https://vitalik.ca/general/2021/04/07/sharding.html">Sharding</a> is the approach being taken to improve scalability of Ethereum in it's 2.0 version. This post explains how sharding can be more secure than simply linking multiple chains in a blockchain ecosystem.</p>
<p>&quot;<a href="https://vitalik.ca/general/2021/05/23/scaling.html">The Limits to Blockchain Scalability</a>&quot; details reasons why making node operation accessible is important for chain security. Includes an analysis of what realistic hardware limits are for node operators before becoming too great of an investment to discourage general participation.</p>
<p>A highly resilient chain archiving mechanism would help especially in the case described below:</p>
<p><em>This is all even more important during an attack, when a successful response to the attack will likely involve many users spinning up new nodes when they were not running nodes before.</em>
{{#include storagebackend_lmdb.md}}</p>
<h2 id="data-deduplication"><a class="header" href="#data-deduplication">Data Deduplication</a></h2>
<p>Data deduplicatio technology has been around for the length of the digital age. We should take a look at what the findings and results are to do reliable, trustworthy and secure backups.</p>
<p>Backup de-duplication was invented in the early 2000's. Up to that point in time datacenters with thousands of servers with the same OS would backup the same systems files a thousand times over. With the rise of the digital age this became very expensive quickly efficiency and competitive drivers forced innovators to start looking at data de-duplication. Once of the earliest implementations are described in this <a href="https://patents.justia.com/patent/7254596">patent</a> which is by the way a patent as done by one of our first companies from our Incubator, please do not we don't believe in patents any more and we don't execute on any of them, we believe the only way forward is opensource. </p>
<p>The use of hashes to comparing local files and backed up files in a central storage facility is a very elegant manner to know if a file (read data) is already backed up. This works not just for one server with files installed but also allows to reach large efficiency gains over a large number of servers. </p>
<p>Excerpt from the patent file:</p>
<p><em>The invention relates to an improvement in backup technology, and more particularly, creates a solution for massive server backup in Internet data center and enterprise data center environments, resulting into a solution for disaster recovery and data protection. The invention is an improved System and a Method of using a hashing key of file content for more efficient and more effective computer file and computer program backups.</em></p>
<p><em>The first step in the process is to scan the file system on the target machine (computer system to be backed up) and creating a hashing key, creating a unique digital code for each of the files to be backed up. In a preferred embodiment, in order to reduce the processing time, a hashing key is only created for the files having a modification date attributed that is more recent than the last backup.</em></p>
<p><em>The resulting hashing keys are stored in a local database—a database on the target computer, for example—for further comparison during the current, and future, backup sessions. The local database also includes the complete path of each backed up file.</em></p>
<p><em>The stored hashing keys are checked against previous hashing key entries in the local database. In this way, the hashing keys are used to check each local file to determine if it was previously backed up on the target system. The hashing keys not found in the local database key list are used in the next step of the process.</em></p>
<p><em>The hashing keys that were not found in the local hashing key database are checked against the hashing keys of the files stored on a central storage server. This check is used to determine if a particular file is already present on the central storage server. The file may be present as the result of a backup from another server or system, or from prior backup operations.</em></p>
<p><em>The decision whether to back up is performed file by file, instead of block-by-block for example. This strongly reduces the number of comparisons and the size of the local database, and is very well adapted to farm servers in which not only data blocks, but often complete files, are duplicated in several servers.</em></p>
<p>This principle, identifying a 'blob' of data by a hash and then checking whether that hash already exists in a data store is a sound principle to also use for blockchains pruning and de-duplication. By definition blockchain nodes will contain the same data on all nodes that are considered to be part of the good nodes. For pruning purposes this data is meant to be stored elsewhere and on the blockchain node itself (which leads to <code>N</code> copies of the same data on all <code>N</code> nodes). The principle describe in the patent paper, to identify a chunk of chain data by a hash and then somehow (to be defined) checking this hash against all the participating nodes seems like a good consensus mechanism that all the <code>blobs</code> of data have the same content. After such proof has been created (and store) the data can be stored remotely is a (to be defined) smart manner.</p>
<p>If we take this principle and apply it to a blockchain and the pruning challenge then, identifying a 'blob' of data by a hash and checking whether that hash already exists in a data store is a sound principle to also use for blockchain pruning and de-duplication.  By definition blockchain nodes will contain the same data on all nodes that are considered to be part of the <em>good</em> nodes.  For pruning purposes this data is meant to be stored elsewhere and on the blockchain node itself (which leads to <code>N</code> copies of the same data on all <code>N</code> nodes).  The principle describe in the patent paper, to identify a chunk of chain data by a hash and then somehow (to be defined) checking this hash against all the participating nodes seems like a good consensus mechanism that all the <code>blobs</code> of data have the same content.  After such proof has been created (and store) the data can be stored remotely is a (to be defined) smart manner.</p>
<p>This technology is battle proven over the last two decades and provides a solid starting point for a blockchain pruning solution. We will come back to this later and use it to build a pruning system.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="offload-blockchain-databases"><a class="header" href="#offload-blockchain-databases">Offload blockchain databases</a></h1>
<p>We did extensive research to try to reduce bandwidth requirement to get a working node running quickly using our quantum same storage system.</p>
<p>The blockchains we experimented with:</p>
<ul>
<li>Casper Labs</li>
<li>Cosmos</li>
<li>Harmony Blockchain </li>
<li>Substrate based blockchains</li>
<li>Most Ethereum based blockchains</li>
<li>Pokt.Network</li>
</ul>
<p>We also researched lots of other blockchains, to see where it would work.</p>
<blockquote>
<p>The research took a lot of time,  we only put some conclusions into this document.</p>
</blockquote>
<h2 id="setup"><a class="header" href="#setup">Setup</a></h2>
<p>In order to get a node working, documentation ask you to clone a copy of the current blockchain database state using <code>rclone</code> and
grabbing data from a <code>AWS</code> storage source.</p>
<p>I did it on my machine with a <code>400 Mbps</code> download line, it took <strong>50 min</strong> to download the whole <code>40 GB</code> database (testnet), which make an average
download speed of <code>14 MB/s</code>.</p>
<p>I've put a copy of that database into a local <code>zdbfs</code> and mounted an overlayfs on top, then used that overlayfs as endpoint for Harmony database.
With this approch, Harmony will read data from <code>zdb/zdbfs</code> but write changes into a classic local storage space, <code>zdbfs</code> is thus used as read-only source
and local space will only be used when required (updated files).</p>
<h2 id="numbers"><a class="header" href="#numbers">Numbers</a></h2>
<p>The database get cloned <strong>4 days</strong> before the day I did the real test, let's assume database was <strong>up-to-date</strong> when cloned, when running the node,
it would need to sync <strong>4 days</strong> of new data. When starting the node with local classic storage with the database, node boot up in <strong>~15-20 seconds</strong> (on <code>SSD</code>).</p>
<p>When using <code>zdbfs</code>, node boot up in <strong>~30 seconds</strong> and read about <code>15 GB</code> of data from <code>zdbfs</code> before being ready. Overlayfs at that time is nearly empty, it act more
like a passthru to <code>zdbfs</code>. After <strong>2 days</strong>, Harmony were fully synced and <code>1.5 TB</code> were read from zdbfs. Local storage space used is about <code>11 GB</code>.</p>
<p>More test will be made to get better values and more deterministics details, theses values were fetched from <code>zdb</code> statistics at differents day moment.</p>
<h2 id="benefits-1"><a class="header" href="#benefits-1">Benefits</a></h2>
<p>According to theses values, an average bandwidth of <code>10 MB/s</code> is required between the node and <code>zdb</code> where database is, with only <code>1 zdb</code> in a local network sharing the database
we can feed lot of node without the need to copy the full database locally, since after <strong>couple of days</strong> running, only <code>15 GB</code> of data were allocated on the node itself, this <strong>reduce the storage needed by 4 per node</strong> and any node can boot in less than <strong>1 min</strong>.</p>
<h2 id="conclusion-1"><a class="header" href="#conclusion-1">CONCLUSION</a></h2>
<blockquote>
<p>THE BENEFIT IS INCREDIBLE</p>
</blockquote>
<p>We have been able to demonstrate how we can do thing provisioning on most blockchain databases.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="poktnetwork"><a class="header" href="#poktnetwork">Pokt.Network</a></h1>
<p>Is a deployment solution for blockchains which demonstrates how thin provisioning works using ThreeFold Quantum Safe Storage FileSystem</p>
<p><img src="research/img/pokt.png" alt="" /></p>
<p>As part of this research we have developed a deployment on Threefold grid to deploy pokt.</p>
<p><img src="research/img/pokt_play.png" alt="" /></p>
<p>This was lots of work for our engineering team but now has now been integrated in our development branch ThreeFold Grid.</p>
<p>Its now possible for people to deploy Node Pilot on ThreeFold Grid.</p>
<h2 id="we-have-been-able-to-demonstrate"><a class="header" href="#we-have-been-able-to-demonstrate">We have been able to demonstrate:</a></h2>
<ul>
<li><strong>We can deploy a full deployer in &lt; 30 minutes rather than days</strong></li>
</ul>
<p>This is amazing news it demonstrates how quantum safe filesystem is a solution for deployment.</p>
<p>We still need to do some hardening to let this work on more blockchains.</p>
<p>We have been able to test this on all blockchains as port of Pokt.Network, we recomment everyone to play with this.</p>
<blockquote>
<p>to try yourself see: https://play.dev.grid.tf/</p>
</blockquote>
<h2 id="nodepilot"><a class="header" href="#nodepilot">Nodepilot</a></h2>
<p>To make it easier for people to get started we used nodepilot</p>
<p><img src="research/img/nodepilot.png" alt="" />  </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="casper-uses-lmdb"><a class="header" href="#casper-uses-lmdb">Casper uses LMDB</a></h1>
<p>CasperLabs uses <a href="https://medium.com/casperlabs/casperlabs-releases-node-0-3-c3a1adb2d645">LMDB</a> which provides some features we can tap into to provide pruning support.</p>
<p>LMDB is a big file on a filesystem and as such its hard to use some storage tricks on the quantum safe filesystem to implement pruning on that level.</p>
<p>To succeed with pruning on casperlabs we have to integrate in the storage layer itself, probably in the code of casperlabs itself which complicates this project.</p>
<h3 id="lightning-memory-mapped-database-manager-lmdb"><a class="header" href="#lightning-memory-mapped-database-manager-lmdb">Lightning Memory-Mapped Database Manager (LMDB)</a></h3>
<p><em>LMDB is a Btree-based database management library modeled loosely on the BerkeleyDB API, but much simplified. The entire database is exposed in a memory map, and all data fetches return data directly from the mapped memory, so no malloc's or memcpy's occur during data fetches. As such, the library is extremely simple because it requires no page caching layer of its own, and it is extremely high performance and memory-efficient. It is also fully transactional with full ACID semantics, and when the memory map is read-only, the database integrity cannot be corrupted by stray pointer writes from application code.</em></p>
<p><em>The library is fully thread-aware and supports concurrent read/write access from multiple processes and threads. Data pages use a copy-on- write strategy so no active data pages are ever overwritten, which also provides resistance to corruption and eliminates the need of any special recovery procedures after a system crash. Writes are fully serialized; only one write transaction may be active at a time, which guarantees that writers can never deadlock. The database structure is multi-versioned so readers run with no locks; writers cannot block readers, and readers don't block writers.</em></p>
<p><em>Unlike other well-known database mechanisms which use either write-ahead transaction logs or append-only data writes, LMDB requires no maintenance during operation. Both write-ahead loggers and append-only databases require periodic checkpointing and/or compaction of their log or database files otherwise they grow without bound. LMDB tracks free pages within the database and re-uses them for new write operations, so the database size does not grow without bound in normal use.</em></p>
<p><em>The memory map can be used as a read-only or read-write map. It is read-only by default as this provides total immunity to corruption. Using read-write mode offers much higher write performance, but adds the possibility for stray application writes thru pointers to silently corrupt the database. Of course if your application code is known to be bug-free (...) then this is not an issue.</em></p>
<p>See on internet: <a href="http://www.lmdb.tech/doc/index.html">LMDB</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="casperlabs-storage-integration-qsfs"><a class="header" href="#casperlabs-storage-integration-qsfs">Casperlabs Storage Integration QSFS</a></h1>
<p>Casper uses <a href="http://www.lmdb.tech/doc/index.html">lmdb</a> for the persistent storage in their blockchain. Lmdb,
an in process key-value store, saves its data in <strong>a single file</strong> on
disk. Since there is ever only 1 data file, and this data file is not
changed (i.e. it is always modified), this rules out the usage of
0-db-fs / 0-stor to run (part of) the database.</p>
<p>The next best option is to analyze what data is stored in the DB.
Mostly, we are interested in block information, as this is the main
source of historic data. The idea would be to periodically remove &quot;old&quot;
blocks from the DB, and move it to a secondary DB. If the amount of
blocks stored in such a secondary instance is limited, this effectively
creates a list of read-only (as blocks, once accepted and written, will
never change) secondary instance data files. These files can then be
written to 0-db-fs / 0-stor once they are finished, and removed locally.
If historical data is needed from these blocks, the files can be
recovered and read.</p>
<p>It should be noted the lmdb storage format is architecture dependant
(x86 vs x86_64, little endian vs big endian). Furthermore, there are no
guarantees about the internal layout of the data file in lmdb. As such,
we can't really use lmdb for this, convenient as that would be. This can
be solved however by creating a deterministic storage format for the
data, and then writing this to a file. For example, we can simply use
<a href="https://github.com/bincode-org/bincode">bincode</a>, a rust binary encoder, to encode blocks, and write the
results to the file. Given the same settings, which we can set
statically in the software, the same result will be produced.
Furthermore, we can statically agree to create such a file every e.g.
10K blocks (amount needs some research regarding block size). This will
cause the same file to be generated on every node. This file can then be
encoded and uploaded.</p>
<p>All of the above does require serious protocol support. Most notably,
there needs to be consensus over which blocks to write to secondary
storage and when. For the main chain, this is not a problem, as every
node is expected to have these blocks (the exception would be temporary
small forks, if this would be possible. For this reason blocks to be
saved need to be far enough in the past). It is harder to also agree on
orphaned blocks, without having some kind of active consensus mechanism.
Since orphaned blocks are persisted in storage, they should eventually
be moved to this secondary storage, however not all nodes will have this
orphaned block, as some nodes might, e.g. have joined the network after
this block was broadcast. This requires more investigation in the casper
consensus protocol.</p>
<p>In this scenario, it seems best to run directly against 0-stor, as we
would upload single files. Furthermore, 0-stor can be modified to return
the metadata, and this metadata could then be broadcast on chain (through
means of a transaction?). This makes sure every node has the metadata
available to recover the uploaded file locally. (TODO: how to reach
consensus over which blocks to store and what 0-dbs to use, who does
upload, ...). Since both 0-stor and the casper blockchain are in rust,
we can also opt to embed 0-stor (as we only need the core
functionality).</p>
<blockquote>
<p>todo: research storage layer of casper, put all findings here
=======</p>
</blockquote>
<h2 id="aim"><a class="header" href="#aim">Aim</a></h2>
<p>The aim is to provide a storage solution, which manages to </p>
<ul>
<li>efficiently store historical data (with regards to used space)</li>
<li>allows the data to be retrieved even when nodes are no longer reachable, or loose access to the data (intentional or accidental).</li>
</ul>
<h2 id="solution--lmdb-integrated-with-0-stor"><a class="header" href="#solution--lmdb-integrated-with-0-stor">Solution : LMDB integrated with 0-stor</a></h2>
<p>At Threefold, we already developed a solution which achieves most of this in <a href="https://github.com/threefoldtech/0-stor_v2">0-stor</a>. This allows storing files in a distributed, redundant fashion, using <code>forward-looking error correcting</code> codes. Considering the existing storage layer in the <a href="https://github.com/casper-network/casper-node">casper node</a>, we feel that integration and interoperation between 0-stor and Casper storage is a viable solution to the given problem.
<a href="https://github.com/threefoldtech/0-stor_v2">0-stor</a> is built to reliably store files with minimal overhead. Even though it can be run as a daemon in stand-alone mode, Threefold has developed a filesystem on top of it. An actual integration seems to be the best way forward: this allows the chain itself to drive the actual storage process. Considering that the existing storage layer uses <a href="http://www.lmdb.tech/doc/index.html">lmdb</a>, which is a single file embedded datastore, we will propose some modifications to this storage layer, to allow the usage of <a href="https://github.com/threefoldtech/0-stor_v2">0-stor</a>.</p>
<ul>
<li><input disabled="" type="checkbox" checked=""/>
what is size of files -&gt; lmdb uses sparse files, on my desktop at
home the database file has a registered size of 750GB at startup</li>
<li><input disabled="" type="checkbox" checked=""/>
what is min file, max -&gt; Not relevant</li>
<li><input disabled="" type="checkbox"/>
performance requirements </li>
<li><input disabled="" type="checkbox" checked=""/>
what is data structure -&gt; Single file</li>
<li><input disabled="" type="checkbox" checked=""/>
how to manipulate positioning of the bigger files -&gt; Not relevant</li>
<li><input disabled="" type="checkbox" checked=""/>
are the large blocks the same ? -&gt; Not relevant</li>
<li><input disabled="" type="checkbox"/>
need to know how consensus has been achieved
=======</li>
</ul>
<h3 id="limit-to-historic-state"><a class="header" href="#limit-to-historic-state">Limit to historic state</a></h3>
<p>First, it should be noted that <a href="https://github.com/threefoldtech/0-stor_v2">0-stor</a> is <em>not</em> intended to be a high speed storage solution, but rather, reliable and space efficient. As such, we can't store everything.
If we identify the storage at some block, then we can split this into an active and a historic storage part. We consider the active state to be the state which was either recently created, or can be accessed as a result of normal operation of the network. This includes recent blocks, recently deployed contracts, ...
Historic data is composed of data which is no longer accessed by the network itself, such as blocks which were created some time ago. Considering the speed restriction, we will only consider historic state for processing.</p>
<p>=======</p>
<h3 id="file-encoding"><a class="header" href="#file-encoding">File encoding</a></h3>
<p>Since Casper's current blockchain storage implementation is based on <a href="http://www.lmdb.tech/doc/index.html">lmdb</a>, which operates on a
single data file, the option of encoding database files directly (should that be possible given the previously stated constraint) is ruled out. </p>
<p>To allow <a href="https://github.com/threefoldtech/0-stor_v2">0-stor</a> operation, the following addition can be made. After every new block is stored,
an older block (in practice, a block which is a fixed height behind the new block) is read from storage, written to a file, and deleted from storage. Every x amount of blocks, the storage file is encoded and uploaded, and a new storage file is started. This also means that a storage file has a fixed amount of blocks. The encoding of the data needs to be deterministic, so that all the nodes in the chain participating in the storage protocol have the exact same data. The above is specific for blocks, but can be used for other data as well. 
It should be pointed out that a node can't actually delete the data, as it needs to retain some way of knowing where it is. Multiple mechanisms can be used for this. For good operation with <a href="https://github.com/threefoldtech/0-stor_v2">0-stor</a> however the file name needs to be deterministic. 
Besides this, the value in the database can be converted to point to this file name, which allows
the storage layer to know that it needs to fetch the actual value from an encoded blob. Depending on the data and structuring, more efficient methods are also possible, which simply indicate that some data is in a blob. For instance, if a fixed amount of blocks is encoded per file, the file name can be used to indicate which blocks are in it based on their respective heights in the chain. It is then sufficient to merely indicate in the database that a block is off-chain.</p>
<h3 id="optional-encryption"><a class="header" href="#optional-encryption">Optional encryption</a></h3>
<p><a href="https://github.com/threefoldtech/0-stor_v2">0-stor</a> is build to support encryption of data before encoding it and uploading the chunks. Since this will generally be used for publicly known data, a small modification can be made which does not force this behavior. That removes overhead from the encryption step, which is not needed, but still allows it in case someone is running a private chain.</p>
<h3 id="requirements-for-node-implementation"><a class="header" href="#requirements-for-node-implementation">Requirements for Node implementation</a></h3>
<p>We consider the following requirements for any node to store a chunk of data. </p>
<h4 id="incentive"><a class="header" href="#incentive">Incentive</a></h4>
<p>For permissionless blockchains, the node needs to have incentive to do so, as storage uses disk space, which has a real world cost. A simple way to achieve this is to have a periodic payout for nodes which store data </p>
<ul>
<li>through creating of new tokens</li>
<li>by distributing (part of) transaction fees. </li>
</ul>
<h4 id="treatment-of-nodes-behaving-badly"><a class="header" href="#treatment-of-nodes-behaving-badly">Treatment of nodes behaving badly</a></h4>
<p>Secondly, a node which has some data stored needs to make sure this is accessible for everyone, and that the data is not lost or otherwise corrupted. For a node to enforce this, the chain needs some way to punish the node if the condition is failed. One approach is to only allow nodes which have some amount of tokens staked participate in data archival, where the stake can be slashed in case a violation is detected. In this scheme, a node should only be able to unstake if it has no data stored. To accomplish this, a node needs the ability to step down as an archiver, which then causes the chain to elect a new node to store the data on. Once the new archiver indicates that the data is stored, the old node's unstaking completes. At this time, we believe that this consensus scheme can be tied into the existing consensus scheme for block validation. When a node stakes, it can indicate whether it wants to archive or not.</p>
<h4 id="election"><a class="header" href="#election">Election</a></h4>
<p>After every x amount of blocks, when the storage file is rotated, the chain elects an ordered set of nodes out of all potential archivers. The election mechanism itself does not matter much. An option could be a <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14757/13791">modified Phragmén election</a>. Note that it is possible to run the election offline, and only commit the results on chain.
This might be needed, as the algorithm takes a noticeable amount of time to finish, which is impractical in the time constraint context of block creating. 
Once the election has ended, every elected node can use <a href="https://github.com/threefoldtech/0-stor_v2">0-stor</a> to compute the storage chunks. They then archive the chunk corresponding to their ranking in the election, and publish the metadata of the storage on chain. This metadata includes the hash of the chunk.
Since every archiver needs to compute every chunk, they can all verify each other's hashes and make sure the data is saved correctly.</p>
<h4 id="0-db-append-only-key-value-store"><a class="header" href="#0-db-append-only-key-value-store">0-db append-only key-value store</a></h4>
<p><a href="https://github.com/threefoldtech/0-stor_v2">0-stor</a> saves data to <a href="https://github.com/threefoldtech/0-db">0-db</a>, an append-only key-value store. This is ideal, as data written is permanent and will never change. The info needed to connect to the right namespace is included in the metadata (IP:port and namespace name). This allows anyone to fetch the chunks for a particular data blob and reconstruct it locally.
In case a <a href="https://github.com/threefoldtech/0-db">0-db</a> needs to have scheduled downtime, or otherwise needs to be replaced, a node can spin up a new one, copy the data from the original one, and finally update the metadata on-chain. They can then safely remove the old db, without getting their funds slashed. Similarly, if another node takes ownership of a data shard because the original archiver wants to unstake, they can simply read the data from storage, verify the checksum, and write it to their own <a href="https://github.com/threefoldtech/0-db">0-db</a>. The transfer is finished
by uploading the new metadata to the chain.</p>
<h4 id="metadata"><a class="header" href="#metadata">Metadata</a></h4>
<p>It is worth noting that this approach can be used to offload metadata storage itself, should this ever be required. In that case, metadata pieces can also be stored in a file, which is then encoded and uploaded. If a node wants to fetch a particular piece of information, it first fetches the metadata blob, reconstructs it, and then uses the contained data to fetch and decode the blob with the actually needed info.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="proposed-solution-methods-to-architect-a-pruning-solution"><a class="header" href="#proposed-solution-methods-to-architect-a-pruning-solution">Proposed solution methods to architect a pruning solution</a></h2>
<blockquote>
<p>TODO: something went wrong in porting the data, we need to go back to engineers and find the images + update the document</p>
</blockquote>
<h3 id="introduction"><a class="header" href="#introduction">Introduction</a></h3>
<p>For many blockchain nodes every (full / validator) node that partakes in the blockchain operation run the layer-1 blockchain software which is part of the blockchain operations.</p>
<p>At this point in time we leave the complexities out that come with the the blockchain being permissionless or permissioned.  This has major impact on how nodes build trust between themselves and the resulting consensus mechanism that operates the blockchain protocol.</p>
<p>For this part of the research we are going to focus on how these blockchain nodes store that data after consensus and trust has been build between all participating nodes.</p>
<h3 id="method-1-use-traditional-full-and-incremental-backuparchive-principles-to-backup-a-database-offchain"><a class="header" href="#method-1-use-traditional-full-and-incremental-backuparchive-principles-to-backup-a-database-offchain">Method 1: Use traditional full and incremental backup/archive principles to backup a database offchain</a></h3>
<h4 id="description"><a class="header" href="#description">Description</a></h4>
<p>Any blockchain node uses local available disk drives to write its full chain data and chain state to.  For some (layer-1) protocols a database is used (like <a href="https://github.com/LMDB/bitmonero">Monero</a>) and for others other data formats have been chosen. These formats might be databases, key values stores or other means of putting data in a structured format before committing it to disk.</p>
<p>Popular cryptocurrencies use a mix of LevelDB and BerkeleyDB. High-performance blockchain databases such as BigchainDB and ProvenDB are using MongoDB.  So each blockchain node runs a local database of sort to store its local chain data and indexes it in a certain way to make it searchable and fast responding to queries.</p>
<img src="research/img/blockchain_local_operation.svg" alt="blockchain_pruning_option_1" width="800"/>
<p>In such a setup one can look at database specific export or backup features to partial exports and backups to store a part af the chain data off-node.</p>
<img src="research/img/blockchain_local_operation_pruning.svg" alt="blockchain_pruning_option_1" width="800"/>
<p>In this method the blockchain node database is used to create a periodic export of the database and all of it's new stored blocks and it is store on a local fuse based file system than has a local storage devices for physical storage.</p>
<p>The quantum safe filesystem has hooks built in to engage a forward looking error correcting coding engine that takes the new data, create data fragments from it, compresses and encrypts tha fragments and then creates a mathematical description of these fragments (plus creates mode mathematical descriptions based on the same compressed and encrypted fragments to create redundancy).  For a more detailed description how this works please see <a href="research/../technology/qsss/qss_algorithm.html">here</a>.</p>
<p><strong>Link To Requirements</strong></p>
<table><thead><tr><th style="text-align: left">Requirement</th><th style="text-align: center">Achieved?</th><th style="text-align: center">Remarks</th></tr></thead><tbody>
<tr><td style="text-align: left">pruning solution should be able to run <em>on</em> node as well as <em>off</em> node by using largely the same method and software</td><td style="text-align: center">yes</td><td style="text-align: center">1.</td></tr>
<tr><td style="text-align: left">the pruning solution should be able to work using the blockchain nodes local storage capacity only and store de-duplicated chain data on these nodes resulting in an over all lower amount of total disk space usage used for all <code>N</code> nodes.</td><td style="text-align: center">yes</td><td style="text-align: center">.</td></tr>
<tr><td style="text-align: left">the pruning solution uses a trusted storage facility that uses external storage capacity to store de-duplicated chain data and is (possibly) governed by a DAO</td><td style="text-align: center">No</td><td style="text-align: center">2.</td></tr>
<tr><td style="text-align: left">the pruning solution should store the chain data such that it provides a proof of recovery method</td><td style="text-align: center">TBD</td><td style="text-align: center">3.</td></tr>
<tr><td style="text-align: left">the pruning solution should transport data (by network) <em>off</em> node in such a way that it cannot be stopped by entities (read ISP's, governments and other legal entities)</td><td style="text-align: center">Yes</td><td style="text-align: center">.</td></tr>
<tr><td style="text-align: left">the pruning solution should allow node operators to <em>opt in</em> and <em>opt out</em> of using it.</td><td style="text-align: center">Yes</td><td style="text-align: center">.</td></tr>
</tbody></table>
<p><em>Remarks</em></p>
<ol>
<li>this methods uses all of the participating node to run software and store data. The fragment dispatcher and dedupe process creates consensus on which fragment is stored on what node and creates meta data to be able to retrieve the de-duplicated DB export container chain data.</li>
<li>the external storage facility to the node are all the other blockchain nodes <code>N-1</code>.  Since the blockchain protocol builds trust and consensus the trust is implicit here.</li>
<li>the proof of recovery method is working for a file based storage system build with this technology where there is a single data injection point.  In this use case there are <code>N</code> data injection points which is theoretically inject the same data. The proof algorithm needs to be build but research shows that this can be done.</li>
</ol>
<p><strong>Necessary conditions</strong> </p>
<table><thead><tr><th>Nr.</th><th>Necessary condition</th><th style="text-align: center">Achieved?</th><th style="text-align: center">Remarks</th></tr></thead><tbody>
<tr><td>1</td><td>Secure, autonomous, decentralized and distributed data processing and storage utility</td><td style="text-align: center">Yes</td><td style="text-align: center">1.</td></tr>
<tr><td>2</td><td>Immutable and always-append storage device</td><td style="text-align: center">Yes</td><td style="text-align: center">2.</td></tr>
<tr><td>3</td><td>Encrypted secure networking</td><td style="text-align: center">Yes</td><td style="text-align: center">3.</td></tr>
<tr><td>4</td><td>A filesystem that presents remote storage to blockchain node</td><td style="text-align: center">Yes</td><td style="text-align: center">4.</td></tr>
<tr><td>5</td><td>Secure access to remote stored and de-duplicated data</td><td style="text-align: center">Yes</td><td style="text-align: center">No</td></tr>
<tr><td>6</td><td>Proof of recoverability</td><td style="text-align: center">Yes</td><td style="text-align: center">5.</td></tr>
</tbody></table>
<p><em>Remarks</em></p>
<ol>
<li>The ThreeFold stack is version 3 and has a proven track record of providing decentralised and distributed data processing and storage utility.</li>
<li>This is what zero-DB has been designed to do. Please find more information <a href="https://github.com/threefoldtech/0-db">here</a></li>
<li>The ThreeFold planetary network does exactly this. PLease find more information <a href="https://github.com/threefoldtech/TF-NetworkConnector_">here</a></li>
<li>Please find mode information <a href="https://github.com/threefoldtech/quantum-storage">here</a></li>
<li>the proof of recovery method is working for a file based storage system build with this technology where there is a single data injection point.  In this use case there are <code>N</code> data injection points which is theoretically inject the same data. The proof algorithm needs to be build but research shows that this can be done.</li>
</ol>
<h3 id="method-2-install-and-use-additional-software-on-the-node-to-prune-chain-data"><a class="header" href="#method-2-install-and-use-additional-software-on-the-node-to-prune-chain-data">Method 2: Install and use additional software on the node to prune chain data</a></h3>
<p>Method 2 is about integrating the backup tooling into the node software. Fuse file system will be created on a node, and data will be sent to ZSTOR. Then, a proof process is executed to build consensus on the same data that is archived in zstor as the one that was used in the block validation consensus. A block is split up in fragments, parity blocks are added for both the data and for the proofs. Once fragments are created, they are distributed over different nodes: each of p+q nodes stores one of the created fragments, into the zdb present in the node. </p>
<img src="research/img/pruning_block_write_option_1.svg" alt="blockchain pruning option 1" width="800"/>
<p>&lt;<Scott to complete>&gt;</p>
<p><strong>Necessary conditions</strong> </p>
<p><strong>Requirements</strong></p>
<table><thead><tr><th>Requirement</th><th>Achieved?</th><th>Remarks</th></tr></thead><tbody>
<tr><td>pruning solution should be able to run <em>on</em> node as well as <em>off</em> node by using largely the same method and software</td><td>yes</td><td>No</td></tr>
<tr><td>the pruning solution should be able to work using the blockchain nodes local storage capacity only and store de-duplicated chain data on these nodes resulting in an over all lower amount of total disk space usage used for all <code>N</code> nodes.</td><td>yes</td><td>No</td></tr>
<tr><td>the pruning solution uses a trusted storage facility that uses external storage capacity to store de-duplicated chain data and is (possibly) governed by a DAO</td><td>No</td><td>No</td></tr>
<tr><td>the pruning solution should store the chain data such that it provides a proof of recovery method</td><td>TBD</td><td>No</td></tr>
<tr><td>the pruning solution should transport data (by network) <em>off</em> node in such a way that it cannot be stopped by entities (read ISP's, governments and other legal entities)</td><td>Yes</td><td>No</td></tr>
<tr><td>the pruning solution should allow node operators to <em>opt in</em> and <em>opt out</em> of using it.</td><td>Yes</td><td>No</td></tr>
</tbody></table>
<p><strong>Necessary conditions</strong> </p>
<table><thead><tr><th>Nr.</th><th>Necessary condition</th><th>Achieved?</th><th>Remarks</th></tr></thead><tbody>
<tr><td>1</td><td>Secure, autonomous, decentralized and distributed data processing and storage utility</td><td>Yes</td><td>No</td></tr>
<tr><td>2</td><td>Immutable and always-append storage device</td><td>Yes</td><td>No</td></tr>
<tr><td>3</td><td>Encrypted secure networking</td><td>Yes</td><td>No</td></tr>
<tr><td>4</td><td>A filesystem that presents remote storage to blockchain node</td><td>Yes</td><td>No</td></tr>
<tr><td>5</td><td>Secure access to remote stored and de-duplicated data</td><td>Yes</td><td>No</td></tr>
<tr><td>6</td><td>Proof of recoverability</td><td>Yes</td><td>No</td></tr>
</tbody></table>
<h3 id="method-3-integrate-pruning-software-module-into-permissionless-blockchain"><a class="header" href="#method-3-integrate-pruning-software-module-into-permissionless-blockchain">Method 3: Integrate pruning software module into permissionless blockchain</a></h3>
<p>Method 2 could work well in a permissioned setup. 
Method 3 is an add-on to method 2 to make it also work in a permissionless consensus model. </p>
<p>The problem with permissionless setups is the fact that there is no hard commitment from the validators that they will continue operating, and that a validator expects some incentive in return for the validator services provided. Especially when dealing with historic information, current consensus models only provide in rewards for closing the current blocks, with the 'obligation' to also keep history up. This leads to the high redundancy in storage. Pruning in these setups is only solved creating a hierarchies in the node structure, which is contradictory to the decentralized nature of permissionless blockchains. </p>
<p>We propose a four-step approach for a non-hierarchical pruning protocol that works in a permissionless setup. It involves elections for the storage of historic batches, each time a new batch of blocks is being built. </p>
<h4 id="step-1--consensus"><a class="header" href="#step-1--consensus">Step 1 : Consensus</a></h4>
<p>The first step in the consensus protocol does not change: n nodes agree on validity of blocks. Once consensus is achieved (using whatever consensus protocol like PoS, PoW, ...) records are stored and de-duplicated over all n nodes. 
With one difference, however, which is that the transactions are stored in a Z-Stor dedupe format, over p storage nodes + q redundant ones (typically 20% of overhead) in a Zero-DB format. </p>
<h4 id="step-2--fill-block-batch"><a class="header" href="#step-2--fill-block-batch">Step 2 : Fill block batch</a></h4>
<p>We propose to group a number of blocks, either for an agreed number of blocks. Idea is to come to a sufficient volume to put aside (ex. 30 MB), and keep the transactional history on-chain as long as the agreed block number hasn't been reached. During this period, history is collected in n*(p+q) equal chunks of data.</p>
<h4 id="step-3--batch-closing"><a class="header" href="#step-3--batch-closing">Step 3 : Batch closing</a></h4>
<p>Once the agreed block number has been fully completed (= consensus achieved), the chunks are closed and are ready to be put off-chain. The zstor format ensures that data is immutable, a fingerprint is created and linking information to where the chunks can be found. This information is registered on-chain. </p>
<h4 id="step-4--off-chain-storage-follow-up"><a class="header" href="#step-4--off-chain-storage-follow-up">Step 4 : Off-chain storage follow-up</a></h4>
<p>A few challenges arise with this setup :</p>
<ul>
<li>Data rot can happen, a node can disconnect or other events can happen making that a node starts behaving as a bad actor. </li>
<li>Every time a batch is closed, the metadata describing the location of the historic batches also go off-chain. </li>
</ul>
<p>Both elements require an active follow-up of where historic batches of transactions are to be found is needed. 
This is why the following is proposed:</p>
<ul>
<li>Within each new storage batch process, a process is launched to register the location of historic batches on the new active part of the chain. </li>
<li>Over time, a number of batches are created. Let's call the number of completed batches <code>y</code>. </li>
<li>An election needs to happen of <code>y * (p + q)</code> chunks. During this election, nodes propose the storing of a chunk, with a number of rules: 
<ul>
<li>The number of eligible chunks per batch per node is less than q (and preferably 1), in order to guarantee that the information is decentralized enough to ensure continuity of service in case a node gets disconnected. </li>
<li>A batch can't be closed as long as the election process for each of the <code>y * (p + q)</code> hasn't been completed. </li>
<li>An election process is triggered by 'candidates', launching a 'proof of storage' transaction, indicating the location of the chunk and a fingerprint of the data including recent info (nonce, timestamp, ... ) and proof of authenticity. The election process for a history batch is completed once <code>p + q</code> transactions are selected. </li>
</ul>
</li>
<li>For older chunks, re-election in every new active batch is possible. However, a chunk storage holder should present his 'proof of storage' in every new batch. </li>
<li>In a permissioned model, the election of new chunk holders is part of the authority rights, and no incentive is to be foreseen. For permissionless models, an incentivization mechanism needs to be worked out for every new election. </li>
</ul>
<h4 id="how-to-split-up-the-data-chunks-when-a-batch-has-just-been-completed-"><a class="header" href="#how-to-split-up-the-data-chunks-when-a-batch-has-just-been-completed-">How to split up the data chunks when a batch has just been completed ?</a></h4>
<p>At batch completion, there are logically n*(p+q) chunks available. However, this completed batch can't be pruned until the completion of the next batch, during which election happens of p+q 'historic chunk batch holders'. 
Number of chunks for every election needed is p+q, with p and q natural numbers and p&gt;1, q&gt;0. q=0 is not viable, as data rot might occur, so the idea is that there is always an objective to keep p+q chunks available, and when a chunks gets unreachable, a new chunk is being created.</p>
<p>The intention is to have (p+q) chunks available at any moment in time, for each batch, hence the re-election of every chunk for every new batch. The keeping of 1 or more of these chunks can be incorporated into the validator node code, but will require way less storage volume than the current full nodes. Moreover the storage can be kept off-chain, as an 'active' transaction archive. </p>
<h4 id="how-to-recollect-the-pieces-if-a-historical-transaction-is-to-be-recovered-"><a class="header" href="#how-to-recollect-the-pieces-if-a-historical-transaction-is-to-be-recovered-">How to recollect the pieces if a historical transaction is to be recovered ?</a></h4>
<ul>
<li>In every of the y completed storage batches, all info is available as a transaction to recover y * (p + q) chunks, and with this info the full transaction history can be recovered, using y * p chunks and y * q spare ones. </li>
</ul>
<p><strong>Requirements</strong></p>
<table><thead><tr><th>Requirement</th><th>Achieved?</th><th>Remarks</th></tr></thead><tbody>
<tr><td>pruning solution should be able to run <em>on</em> node as well as <em>off</em> node by using largely the same method and software</td><td>yes</td><td>No</td></tr>
<tr><td>the pruning solution should be able to work using the blockchain nodes local storage capacity only and store de-duplicated chain data on these nodes resulting in an over all lower amount of total disk space usage used for all <code>N</code> nodes.</td><td>yes</td><td>No</td></tr>
<tr><td>the pruning solution uses a trusted storage facility that uses external storage capacity to store de-duplicated chain data and is (possibly) governed by a DAO</td><td>No</td><td>No</td></tr>
<tr><td>the pruning solution should store the chain data such that it provides a proof of recovery method</td><td>TBD</td><td>No</td></tr>
<tr><td>the pruning solution should transport data (by network) <em>off</em> node in such a way that it cannot be stopped by entities (read ISP's, governments and other legal entities)</td><td>Yes</td><td>No</td></tr>
<tr><td>the pruning solution should allow node operators to <em>opt in</em> and <em>opt out</em> of using it.</td><td>Yes</td><td>No</td></tr>
</tbody></table>
<p><strong>Necessary conditions</strong> </p>
<table><thead><tr><th>Nr.</th><th>Necessary condition</th><th>Achieved?</th><th>Remarks</th></tr></thead><tbody>
<tr><td>1</td><td>Secure, autonomous, decentralized and distributed data processing and storage utility</td><td>Yes</td><td>No</td></tr>
<tr><td>2</td><td>Immutable and always-append storage device</td><td>Yes</td><td>No</td></tr>
<tr><td>3</td><td>Encrypted secure networking</td><td>Yes</td><td>No</td></tr>
<tr><td>4</td><td>A filesystem that presents remote storage to blockchain node</td><td>Yes</td><td>No</td></tr>
<tr><td>5</td><td>Secure access to remote stored and de-duplicated data</td><td>Yes</td><td>No</td></tr>
<tr><td>6</td><td>Proof of recoverability</td><td>Yes</td><td>No</td></tr>
</tbody></table>
<h3 id="method-4-use-an-external-decentralized-storage-facility-and-uses-chain-consensus"><a class="header" href="#method-4-use-an-external-decentralized-storage-facility-and-uses-chain-consensus">Method 4: Use an external (decentralized storage facility and uses chain consensus)</a></h3>
<p>The most elegant solution would be to have the blockchain node
<img src="research/../img/pruning_block_write_option_2.svg" alt="blockchain pruning option 2" width="800"/></p>
<p><strong>Requirements</strong></p>
<table><thead><tr><th>Requirement</th><th>Achieved?</th><th>Remarks</th></tr></thead><tbody>
<tr><td>pruning solution should be able to run <em>on</em> node as well as <em>off</em> node by using largely the same method and software</td><td>yes</td><td>No</td></tr>
<tr><td>the pruning solution should be able to work using the blockchain nodes local storage capacity only and store de-duplicated chain data on these nodes resulting in an over all lower amount of total disk space usage used for all <code>N</code> nodes.</td><td>yes</td><td>No</td></tr>
<tr><td>the pruning solution uses a trusted storage facility that uses external storage capacity to store de-duplicated chain data and is (possibly) governed by a DAO</td><td>No</td><td>No</td></tr>
<tr><td>the pruning solution should store the chain data such that it provides a proof of recovery method</td><td>TBD</td><td>No</td></tr>
<tr><td>the pruning solution should transport data (by network) <em>off</em> node in such a way that it cannot be stopped by entities (read ISP's, governments and other legal entities)</td><td>Yes</td><td>No</td></tr>
<tr><td>the pruning solution should allow node operators to <em>opt in</em> and <em>opt out</em> of using it.</td><td>Yes</td><td>No</td></tr>
</tbody></table>
<p><strong>Necessary conditions</strong> </p>
<table><thead><tr><th>Nr.</th><th>Necessary condition</th><th>Achieved?</th><th>Remarks</th></tr></thead><tbody>
<tr><td>1</td><td>Secure, autonomous, decentralized and distributed data processing and storage utility</td><td>Yes</td><td>No</td></tr>
<tr><td>2</td><td>Immutable and always-append storage device</td><td>Yes</td><td>No</td></tr>
<tr><td>3</td><td>Encrypted secure networking</td><td>Yes</td><td>No</td></tr>
<tr><td>4</td><td>A filesystem that presents remote storage to blockchain node</td><td>Yes</td><td>No</td></tr>
<tr><td>5</td><td>Secure access to remote stored and de-duplicated data</td><td>Yes</td><td>No</td></tr>
<tr><td>6</td><td>Proof of recoverability</td><td>Yes</td><td>No</td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><h1 id="suggestion-solution"><a class="header" href="#suggestion-solution">Suggestion Solution</a></h1>
<p>Implementing good enough pruning for Casper Labs is not a trivial task.</p>
<p>We believe that we have to put it in 2 phases, 1 phase is for research and proof of concept which can be used generically and proves the concept of a global pruning layer.</p>
<p>The 2nd phase should be for another grant of other collaboration effort to put it inside the storage engine of Casper Blockchain and work closely together with the engineers of Casperlabs.</p>
<h2 id="phase-1-solution-this-grant"><a class="header" href="#phase-1-solution-this-grant">Phase 1 Solution (this grant)</a></h2>
<p>A pruning DB, writing on global level to hundreds of backends in such a way the performance stays high, the data cannot be corrupted, the data can be self-healed when needed. Demonstrate how a pruning DB can be a super efficient backend storage layer for a blockchain like Casper Labs.</p>
<p><strong>Solution see <a href="solution/architecture.html">architecture</a></strong>.</p>
<h3 id="proof-points"><a class="header" href="#proof-points">Proof Points</a></h3>
<ul>
<li>create a global accesible data lake for a blockchain which is deployed in 100 locations and only 20 nodes are needed to retrieve the data.</li>
<li>Prove that 80 nodes can be offline and data can still be retrieved.</li>
<li>Prove that 100 TB of data can be stored this way using 1 Pruning DB.</li>
<li>Make test scripts which show how performance is high for the key value stor and stays high independent of size of DB. We will test with 100 TB.</li>
<li>Show how multiple Prune DB's at the same DB can read the data when not in cache from the same data lake.</li>
<li>prove that 100 MB/sec can be achieved towards backend (if enough bandwidth available)</li>
<li>show how data gets encrypted and distributed in such a way that even for a serious hacker it would be hard to do something with the data.</li>
<li>this effort goes together with another grant which is the deployment of such a system on top of TFGrid in all scalability and with a consensus driven deployment mechanism.</li>
<li>All the code delivered is opensource.</li>
</ul>
<p>{{#include ../grants_future.md}}</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="architecture-this-grant"><a class="header" href="#architecture-this-grant">Architecture (this grant)</a></h2>
<pre class="mermaid">graph TD
    A[Blockchain Engine] --&gt;|Offload Data| B(Prune DB)
    B --&gt;|WRITE PATH| C{TF FLECC Codec}
    C --&gt;|One| D[NODE 1]
    C --&gt;|Two| E[NODE 2]
    C --&gt;|Three| F[NODE 3]
    C --&gt;|Hundred| G[... 100 NODES]
</pre>
<p>For write path we write to e.g. 100 nodes, all over the world. This amount is configurable and also the storage policy used.</p>
<p>Default we could use like 20 + 80, means min 20 nodes are needed to re-create the original. This means upto 60 nodes can be lost before data is lost.</p>
<h2 id="the-prune-db"><a class="header" href="#the-prune-db">The Prune DB</a></h2>
<p>Is a fast key value stor which can be used as backend for the storage engine of the blockchain for pruning support.</p>
<p>This key value stor will write in local DB's (storage containers see further) which are configurable in size and can be possitioned on storage system of choice, we recommend SSD.</p>
<p>The storage containers are cacheable, once written they can be removed, if data is needed from a contaier it will be fetched back from the FLECC CODEC in a redundant way. This will make the Prune DB caching aware and can support petabytes easily.</p>
<pre class="mermaid">graph TD
    A[Blockchain Engine] --&gt;|Offload Data| B(Prune DB)
    B --&gt;|Data| C(Storage Container 100 MB)
    B --&gt; D(Storage Container 100 MB)
    B --&gt; E(Storage Container 100 MB)
    B --&gt; X(metadate engine)
    X --&gt; F(Metadata DB 1 MB)
    X --&gt; G(Metadata DB 1 MB)
    X --&gt; H(Metadata DB 1 MB)

</pre>
<p>Above provides for a very scalable system, the metadata DB's and DATA DB's get offloaded using the FLECC Codec towards potentially thousands of nodes on the backend.</p>
<h2 id="self-healing-this-grant"><a class="header" href="#self-healing-this-grant">Self Healing (this grant)</a></h2>
<p>The system should be full self healing, this means if nodes get offline, or there is data rot (corrupted data), the data needs to be corrected automatically and redistributed so that we get back to the original health of the data in line with original policy.</p>
<h2 id="corruption-proof"><a class="header" href="#corruption-proof">Corruption Proof</a></h2>
<p>Data should not be able to get corrupted, once corruption occurs because of network or storage subsystem the codec needs to be able to recover it right away.</p>
<h2 id="the-write-path-is-redundant"><a class="header" href="#the-write-path-is-redundant">The write path is redundant</a></h2>
<pre class="mermaid">graph TD
    A[Blockchain Engine] --&gt;|Offload Data| B(Prune DB)
    B --&gt;|RAFT CONSENSUS| C{TF FLECC Codec}
    B --&gt;|Only 1 MASTER| D{TF FLECC Codec}
    B --&gt;|CONSENSUS| E{TF FLECC Codec}
    B --&gt;|CONSENSUS| F{TF FLECC Codec}
    B --&gt;|UPTO 9 is ok| G{TF FLECC Codec}

</pre>
<p>The Consensus layer for write ath will be based on Tendermint (or alternative if CasperLabs has suggestions).</p>
<p>The Pruning DB creates storage containers of 1-100 MB in size (configurable). Each Storage Container gets put in separate file if a configurable time interval got passed or a certain size for the storage container, default 100 MB.</p>
<h2 id="redundancy-for-prune-db"><a class="header" href="#redundancy-for-prune-db">Redundancy for Prune DB</a></h2>
<p>We will not make the Pruning DB active-active but this can be done as part of the next grant. For now the pruning DB will write the storage containers with data and metadata. If the node on which we do the activiation of the pruning dies then that pruning action will have to be restarted.</p>
<p>Active-Passive clustering can be build inside the Pruding DB.</p>
<pre class="mermaid">graph TD
    A[Blockchain Engine] --&gt;|Offload Data| B(Master Prune DB)
    B --&gt; |Synchronous need good network| C(Secondary Prune DB)
    B --&gt; D(Secondary Prune DB)

</pre>
<p>Only one pruning DB is active, if the master is down, a Raft mechanism might be used (next grant) to define which DB becomes the new master.</p>
<h2 id="each-zeros-node-can-store-500-tb"><a class="header" href="#each-zeros-node-can-store-500-tb">Each Zeros-Node can store 500 TB.</a></h2>
<pre class="mermaid">graph TD
    F{TF FLECC Codec} --&gt; A(NODE 1 HD ZDB)
    F{TF FLECC Codec} --&gt; B(NODE 1 HD ZDB)
    F{TF FLECC Codec} --&gt; C(NODE 1 HD ZDB)

</pre>
<p>100+ ZDB's can be used by 1 codec, the codec will make sure that data is spread out good enough.</p>
<h2 id="read-path"><a class="header" href="#read-path">Read Path</a></h2>
<p>The read path is different and redundant by design. Thanks to the FLECC Codec we can read data from hundred of sources and this can happen by many readers at the same time.</p>
<pre class="mermaid">graph TD
    P1(Prune DB US) ---|READ| F
    P2(Prune DB CH) ---|READ| F
    P3(Prune DB DUBAI) ---|READ| F
    P4(Prune DB TORONTO) ---|READ| F
    F{TF FLECC Codec} --- A(NODE 1 DUBAI HD ZDB)
    F --- B(NODE 2 BE HD ZDB)
    F --- C(NODE 3 US HD ZDB)

</pre>
<p>This leads to the fastest possible way how to retrieve data so the prune DB can retrieve the data it needs.</p>
<p>Once a storage container is retrieved it will stay in cache untill no longer needed.</p>
<p>The caching layer needs to be intelligent to delete the right files as much as possible.</p>
<h2 id="read-optimization-grant-future"><a class="header" href="#read-optimization-grant-future">Read Optimization (Grant Future)</a></h2>
<p>We can implement a system which will allow a faster retrievel, in the suggested system we only need to retrieve 20 files from the 100 ZDB's (backend DB's), what if the system would be smart enough to figure out which files to retrieve first and depending who answers fastest the data can be rebuild to have back the original data. It could work with e.g 50 of the 100, the first 20 who deliver allow the data to be retrieved, this would dramaically speed up retrieval and data would come from latency locations close by.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quantum-safe-storage-system-benefits"><a class="header" href="#quantum-safe-storage-system-benefits">Quantum Safe Storage System Benefits</a></h1>
<p>ThreeFold would start from our existin QSSS (Quantym Safe Storage System) to deliver what Casper Labs requires, we would use and extend our existing codebase.</p>
<p>QSFS stands for Quantum-Safe File System. It is a redundant storage system, which can store petabytes of information.</p>
<p>Unique features are : </p>
<ul>
<li>Unlimited scalable (many petabytes)</li>
<li>Data is spread over many devices owned by different, independent hardware owners called farmers. Together, these farmers provide the capacity to a hardware grid, call the ThreeFold Grid. </li>
<li>Dispersion over multiple sites ensures 100% privacy of the data, as no farmer knows what the data is about (zero knowledge storage system). Even a quantum computer cannot decrypt the data on a node, as one node contains insufficient information to unambiguously recreate the authentic data.</li>
<li>Data can’t be lost: there is a protection for datarot, data will auto-repair.</li>
<li>Data is append-only and immutable by design of the protocol, so it’s fit for storing ledger history. Even with sites going down, data is not lost with ‘operational’ backup nodes in place.</li>
<li>Up to 10x more efficient than storing on classic storage cloud systems. Overhead of about 20% is sufficient to have a secure archive.</li>
<li>Self-healing: when node or disk lost, storage system can get back to original redundancy level.</li>
<li>Helps with compliance to privacy regulations like GDPR (as the hosting facility has no view on what is stored, information is encrypted and incomplete).</li>
<li>Hybrid : can be installed onsite, public, private, ...</li>
<li>Read-write caching on encoding node (the front end).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="requirements-introduction"><a class="header" href="#requirements-introduction">Requirements Introduction</a></h2>
<p>A blockchain is in essence a ledger of immutable records that are always append and therefore linked and a chain, The total chain forms the &quot;end state&quot;. In order to proof validity of this end state, the complete history needs to be kept from the genesis time of the blockchain to now. </p>
<p>Blockchains enable organizations to run decentralized, they allow value transaction to be done without a third party in the middle and any other use case of blockchain technology always revolves around decentralization.  The permissionless blockchain provide the most pure form of decentralization where node owners and operators decide for themselves whether the reward for operating a node is sufficient for them to get involved (or not).  Permissioned blockchains have a slightly less decentralized character but certainly have a distributed character where the reward is not just governed by an algorithm because here there might be a legal entity involved to orchestrate the rewards system.  This  legal entity might be a DAO (Distributed Autonomous Organization).</p>
<p>The number and types of implementations of blockchains (supporting the growth in adoptions for traditional use cases and  DAO's) is going to grow exponentially, there is a need for level-1 blockchain protocols to be able to deal with the increased volume of transactions and history, making pruning a necessity.</p>
<p>Increase in data volume and transactions volume will create issues at some point in time, a ledger holding the complete chain data will become too big for a key aspects of a blockchain:</p>
<ul>
<li>fast transaction times require minimum synchronization time.</li>
<li>security and proof of validity requires the complete chain to be stored many times over to overcome a 51% attack</li>
<li>decentralized operations is key but operating a full nodes becomes more expensive by excessive chain data growth which in the long run will rule out people that do not have the means to afford such hardware.</li>
</ul>
<p>All of the above threatens the decentralization of any layer-1 blockchain project in general.</p>
<p>From the literature researched we formed ideas how we can come to a &quot;non intrusive&quot; pruning system for regular blockchain protocols without necessarily make major changes to the existing blockchain and consensus algorithms. To form such a solution there are a number of technology components that will make creating such a solution possible or easier to create.  To architect methods to solve this problem we have opted to use the ThreeFold data processing and storage stack which presents a unique decentralized utility / cloud platform with some unique storage capabilities that will make developing this solution possible.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="high-level-requirements"><a class="header" href="#high-level-requirements">High level requirements</a></h2>
<ul>
<li>archive (prune)
<ul>
<li>only keep relevant blockchain data on the validator nodes itself, old data should be archived (pruned)</li>
</ul>
</li>
<li>caching
<ul>
<li>caching of data (only retrieve relevant information when needed)</li>
<li>seamless caching is required, this means that a blockchain node only fetches the data blocks when required</li>
</ul>
</li>
<li>data deduplication
<ul>
<li>make sure data is never stored twice on backend system</li>
</ul>
</li>
<li>proof of recovery
<ul>
<li>the pruning solution should store the chain data such that it provides a proof of recovery method </li>
</ul>
</li>
<li>redundant archive storage paths
<ul>
<li>need to make sure that data can always be stored and cannot be stopped by indivual parties (e.g. hackers)</li>
</ul>
</li>
<li>node operators can choose how much is archived
<ul>
<li>the pruning solution should allow node operators to <em>opt in</em> and <em>opt out</em> of using it.</li>
<li>amount of caching can be selected</li>
</ul>
</li>
<li>self repear
<ul>
<li>make sure if data gets corrupted or there are missing data pieces the data get's repaired automatically</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="necessary-conditions"><a class="header" href="#necessary-conditions">Necessary conditions.</a></h2>
<p>Taking the high level requirements into consideration we take the following necessary conditions on board to design the solution:</p>
<h3 id="necessary-condition-1-nc1-secure-autonomous-decentralized-and-distributed-data-processing-and-storage-utility"><a class="header" href="#necessary-condition-1-nc1-secure-autonomous-decentralized-and-distributed-data-processing-and-storage-utility"><strong>Necessary Condition 1 (NC1)</strong>. <em>Secure, autonomous, decentralized and distributed data processing and storage utility</em></a></h3>
<p>To create autonomous, decentralized and distributed data processing and storage utility we need an operating system that provides maximum security by not allowing people to give it instructions or configure it.  To reach a distributed and decentralized grid of processing and storage capacity we cannot rely on people to install and operate standard operating systems.  A standard operating system would allow individual owners of these nodes to be able to see (and potentially manipulate) the incoming data parts to be stored.</p>
<p>ThreeFold has developed such an operating system that gets its instructions from a decentralized ledger where a smart contract governs the execution of such operational instructions.  Such an operating system would allow the compute and storage nodes involved in the pruning solution to be safe, private, sovereign and autonomous  in it's operations.</p>
<p><strong>Necessary condition 2 (NC2)</strong>. <em>Immutable and always-append storage device</em></p>
<p>Use a low level storage device that uses physical storage similar to how a blockchain create chain data: always append and immutable.  When data is committed to this storage device it cannot be deleted.  The data is committed and stored on this storage device for as long as this device is part of the storage system for pruned data.</p>
<p><strong>Necessary condition 3 (NC3)</strong>. <em>Encrypted secure networking</em></p>
<p>Use secure and encrypted networking between the blockchain nodes and the <em>remote</em> storage utility.  Chain data needs to be transported off node and we need to have certainty that data cannot be changed or messed with in the transport part of the pruning process. Therefore encrypted networking using an overlay network technology is an important component for taking chain data off blockchain nodes.</p>
<p><strong>Necessary condition 4 (NC4)</strong>. <em>A storage system that presents remote storage to blockchain node</em></p>
<p>All activities of storing data away from the blockchain node and creating the necessary proof points along the way should require minimal to no impact on the blockchain software.</p>
<p>Proof point should include:</p>
<ul>
<li>authenticity of being pruned from a particular node</li>
<li>comparing off chain data from different nodes to contain the same data</li>
<li>de-duplicating data</li>
<li>other activities to condense the data footprint and make the whole blockchain more efficient and scalable</li>
</ul>
<p>Some blockchain protocols have built in capabilities that allow partial backup of chain data and provide the required proof of the authenticity of the partial backup which makes the remote storage filesystem an ideal way to take data off-node.</p>
<p><strong>Necessary condition 5 (NC5)</strong> . <em>Secure access to remote stored and de-duplicated data</em></p>
<p>Provide secure access to the stored history of the chains and make all nodes (ideally) use de-duplicated copies of the historic data.</p>
<p><strong>Necessary condition 6 (NC6)</strong>. <em>Proof of recoverability</em></p>
<p>When data is stored off node in a de-duplicated format there should be a regular check if the pruned data is recoverable.  This necessary condition will also create the opportunity for new nodes to join at and start validating and securing from the current chain state while in the background cycling back the the genesis block on the pruned chain data and create proof of recoverability for the while chain. </p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="solution-components"><a class="header" href="#solution-components">Solution components</a></h2>
<p>Here is a brief description of how our suggested meet the necessary conditions.</p>
<h3 id="nc1-secure-and-autonomous-operating-system"><a class="header" href="#nc1-secure-and-autonomous-operating-system">NC1: Secure and autonomous Operating System</a></h3>
<p>The principles to build a secure and autonomous operating system to build a decentralized and distributed grid of capacity are as follows:</p>
<ul>
<li><em>Autonomy</em>: to create compute, storage and networking capacity everywhere you can not rely on a remote (or a local) maintainer of the operating system. Also owners should not have to be operating system administrators.  By making the OS autonomous and not allowing owners or systems administrators to log in to the OS you make it a very secure operating system.</li>
<li><em>Simplicity</em>: An operating system should be simple, able to exist anywhere, for anyone, good for the planet.  Simplicity also enhances the overall security of a system </li>
<li><em>Stateless</em>: In a grid (Peer To Peer) set up, the sum of the components is providing a stable basis for single elements to fail and not bring the whole system down. Therefore, it is necessary for single elements to be stateless, and the state needs to be stored within the grid.</li>
</ul>
<img src="requirements/img/zero_os_overview.jpg" alt="threefold_zero_os_overview" width="800"/>
<p>Building an autonomous, simple and stateless OS is not an easy feat.  Not having access means that is has to be 100% right at time of deployment.  Zero-OS has been developed and improved over the last 5 years and is now ready to be the capacity generator for secure IT workloads where compute, storage and networking components interact.</p>
<p>Thanks to ThreeFold we have this solution available to use in our solution for Casperlabs.</p>
<h3 id="nc2-immutable-and-always-append-storage-protocol"><a class="header" href="#nc2-immutable-and-always-append-storage-protocol">NC2: Immutable and always-append storage protocol</a></h3>
<p>In such an autonomous operating system storing data needs to be done in a very secure manner.</p>
<p>As owners, administrators and users do not have direct access to the operating system a very secure environment is created to run applications and store data.  Also since this operating system is made to form a grid creating ubiquitous compute, storage and network utility local storage devices can be used to make a &quot;dispersed&quot; storage system.</p>
<p>Thanks to ThreeFold we have this solution available to use in our solution for Casperlabs.</p>
<p>Our solution as base for the backend of our dispersed storage system is called zero-DB.</p>
<p>Zero-db is a fast and efficient key-value store (redis-protocol compatible), which makes data persistent inside an always append data file, with namespaces support.  This zero-DB is able to receive and send information over a secure network that spans between all the zero-OS's and as such many zero-DB's can together create a large storage lake.</p>
<p>The Zero-DB stores data like a key-value store, and can operate (when configured to do so) to store data sequentially which makes it an &quot;always append&quot; storage device.  For caching purposes it can also be configured to not do this.</p>
<img src="requirements/img/zdb_arch.jpg" alt="threefold_zdb_arch" width="800"/>
<h3 id="nc3-encrypted-secure-networking-the-planetary-network"><a class="header" href="#nc3-encrypted-secure-networking-the-planetary-network">NC3: Encrypted secure networking: the Planetary Network</a></h3>
<p>The planetary network is an overlay network which lives on top of the existing internet (or any other network created). In this network, everyone can direct connect to everyone and routing does not rely on ISP and Tier 1 providers routing tables. The technology uses a Distributed Hash Table that allows all participating nodes to find the best network path to where it needs to send data. End-to-end encryption between application on the zero-OS's.</p>
<p>Each user and network point is strongly authenticated and uniquely identified, independent of the network carrier used. There is no need for a centralized firewall or VPN solutions, as there is a circle based networking security in place.</p>
<p>Thanks to ThreeFold we have this solution available to use in our solution for Casperlabs.</p>
<p>Benefits :</p>
<ul>
<li>shortest possible paths between peers, independent of the network providers routing decisions</li>
<li>end-to-end encrypted data transport create full security</li>
<li>peer2peer links like meshed wireless</li>
<li>broken internet links do not affect the operating of traffic by re-routing traffic when needed</li>
</ul>
<img src="requirements/img/planet_net.jpg" alt="planetary_network" width="800"/>
<h3 id="nc4-filesystem-that-presents-remote-storage-to-blockchain-node"><a class="header" href="#nc4-filesystem-that-presents-remote-storage-to-blockchain-node">NC4: Filesystem that presents remote storage to blockchain node</a></h3>
<p>Thanks to ThreeFold we have this solution available to use in our solution for Casperlabs.</p>
<p>Quantum Safe Storage System uses a dispersed storage algorithm to distribute the data in a smart way and store data in different locations. The original object is fragmented, compressed and encrypted, and than a &quot;description&quot; is created of that compressed and encrypted fragment which is stored. The original compressed and encrypted data is deleted.  Only that description of that data part  of the information is stored, making it impossible to understand what data(part) is stored on a single device as you need all the descriptions together to be able to &quot;un-describe&quot; the compressed and encrypted original data</p>
<p>The data is described in a way such that a person aiming to hack into the low-level data (which is almost impossible in itself), will only find non-relevant information on this storage infrastructure and the other data shards can’t be re-created, making it quantum-proof.</p>
<p>Quantum Safe Storage System offers the following storage benefits:</p>
<ul>
<li>Store Petabytes of data at hyper-competitive pricing.</li>
<li>Quantum-safe security (not even a quantum computer can hack).</li>
<li>A filesystem interface see Quantum Safe Filesystem</li>
<li>Unlimited scalability provided by the ThreeFold P2P infrastructure.</li>
<li>Self-healing capability of the storage layer ensures your data remains available at all times.</li>
</ul>
<img src="requirements/img/zos_zstor.jpg" alt="zos_zero_store" width="800"/>
<h3 id="nc5-secure-access-to-de-duplicated-data"><a class="header" href="#nc5-secure-access-to-de-duplicated-data">NC5: Secure access to de-duplicated data</a></h3>
<p>ThreeFold Tech has developed the technology to store immutable records in a more space efficient way, relying on a fully decentralized grid of storage capacity. No participant has the full storage volume on his hard drive, however the combination of all participants allows recomposing of the full ledger with all records. This method brings many benefits: </p>
<ul>
<li>The storage happens in a very, quantum-resilient way, as attacking one chunk gives insufficient information to recreate the authentic data;</li>
<li>Storage grows slower over time compared to a classic blockchain setup, as storage overhead can be limited to about 20% instead of the ‘traditional’ 10000% (in the case of 100 participants running a blockchain node) overhead;</li>
<li>It allows for an easy way to implement pruning: only the current state is really required to be stored locally, and when transactions come in, a recalculation is made, and the storing of the historical transactions is directly done using QSFS</li>
</ul>
<h3 id="nc6-proof-of-retrievability-por"><a class="header" href="#nc6-proof-of-retrievability-por">NC6: Proof of Retrievability (POR)</a></h3>
<p>We need to have a way for archived chain to have proof of retrievability (POR). A POR is a protocol in which a server/archive proves to a client that a target file F is intact, in the sense that the client can retrieve all of F from the server with high probability.</p>
<p>For a good working pruning solution POR is a necessary conditions</p>
<p>This needs to be created on top of ThreeFold Quantum Safe Storage System.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="technology"><a class="header" href="#technology">Technology</a></h1>
<p>ThreeFold has developed a highly efficient infrastructure layer for a new internet.</p>
<p>ThreeFold tech stack has 4 main layers</p>
<ul>
<li>compute - a flexible way to deploy any workload on edge cloud computers</li>
<li>storage - a quantum safe storage system</li>
<li>network - a planetary scalable overlay network</li>
<li>blockchain for Smart Contract For IT</li>
</ul>
<p><img src="technology/img/tech_overview2.png" alt="" /></p>
<p>A lot of capacity has been deployed in the world, ThreeFold farmers buy a computer and they connect it to the internet, as such they use our Operating system to provide Internet capacity to the world.</p>
<p><img src="technology/img/3node_simple.png" alt="" /></p>
<p>There are multiple ways how people can interactive without our platform (as developer or IT expert = sysadmin):</p>
<p><img src="technology/img/architecture_usage.png" alt="" /></p>
<p>All technology is developed by ThreeFold and is opensource, this technology is being used for the ThreeFold grid see https://www.threefold.io which is a deployment of a new internet which is green, safe and owned by all of us.</p>
<p>This document explains how we are a missing layer for the full web2, web3 and blockchain world.</p>
<p>This leads to a system which is highly scalable.</p>
<p><img src="technology/img/web_remade.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><!-- ![](img/qsss_intro_.jpg) -->
<p>i<img src="technology/qsss/img/qsss_intro.png" alt="" /></p>
<h1 id="quantum-safe-storage-system"><a class="header" href="#quantum-safe-storage-system">Quantum Safe Storage System</a></h1>
<p>Imagine a storage system with the following benefits</p>
<blockquote>
<p>This is not a dream but does exist and is the underpinning of the TFGrid.</p>
</blockquote>
<p>Our storage architecture follows the true peer-to-peer design of the TF grid. Any participating node only stores small incomplete parts of objects (files, photos, movies, databases...) by offering a slice of the present (local) storage devices. Managing the storage and retrieval of all of these distributed fragments is done by a software that creates development or end-user interfaces for this storage algorithm. We call this '<strong>dispersed storage</strong>'.</p>
<p><img src="technology/qsss/img/qsss_intro_0_.jpg" alt="" /></p>
<p>Peer2peer provides the unique proposition of selecting storage providers that match your application and service of business criteria. For example, you might be looking to store data for your application in a certain geographic area (for governance and compliance) reasons. Also, you might want to use different &quot;storage policies&quot; for different types of data. Examples are live versus archived data. All of these uses cases are possible with this storage architecture and could be built by using the same building blocks produced by farmers and consumed by developers or end-users.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quantum-safe-storage-algoritm"><a class="header" href="#quantum-safe-storage-algoritm">Quantum Safe Storage Algoritm</a></h1>
<p><img src="technology/qsss/img/qss_scaleout.png" alt="" /></p>
<p>The Quantum Safe Storage Algorithm is the heart of the Storage engine.  The storage engine takes the original data objects and creates data part descriptions that it stores over many virtual storage devices (ZDB/s)</p>
<p>Data gets stored over multiple ZDB's in such a way that data can never be lost.</p>
<p>Unique features</p>
<ul>
<li>data always append, can never be lost</li>
<li>even a quantum computer cannot decrypt the data</li>
<li>is spread over multiple sites, sites can be lost, data will still be available</li>
<li>protects for datarot.</li>
</ul>
<h3 id="why"><a class="header" href="#why">Why</a></h3>
<p>Today we produce more data than ever before. We could not continue to make full copies of data to make sure it is stored reliably. This will simply not scale. We need to move from securing the whole dataset to securing all the objects that make up a dataset.</p>
<p>ThreeFold is using space technology to store data (fragments) over multiple devices (physical storage devices in 3Nodes). The solution does not distribute and store parts of an object (file, photo, movie...) but describes the part of an object. This could be visualized by thinking of it as equations.</p>
<h3 id="details"><a class="header" href="#details">Details</a></h3>
<p>Let a,b,c,d.... be the parts of that original object. You could create endless unique equations using these parts. A simple example: let's assume we have 3 parts of original objects that have the following values:</p>
<pre><code>a=1
b=2
c=3
</code></pre>
<p>(and for reference that part of real-world objects is not a simple number like <code>1</code> but a unique digital number describing the part, like the binary code for it <code>110101011101011101010111101110111100001010101111011.....</code>). With these numbers we could create endless amounts of equations:</p>
<pre><code>1: a+b+c=6
2: c-b-a=0
3: b-c+a=0
4: 2b+a-c=2
5: 5c-b-a=12
......
</code></pre>
<p>Mathematically we only need 3 to describe the content (=value) of the fragments. But creating more adds reliability. Now store those equations distributed (one equation per physical storage device) and forget the original object. So we no longer have access to the values of a, b, c and see and we just remember the locations of all the equations created with the original data fragments. Mathematically we need three equations (any 3 of the total) to recover the original values for a, b or c. So do a request to retrieve 3 of the many equations and the first 3 to arrive are good enough to recalculate the original values. Three randomly retrieved equations are:</p>
<pre><code>5c-b-a=12
b-c+a=0
2b+a-c=2
</code></pre>
<p>And this is a mathematical system we could solve:</p>
<ul>
<li>First: <code>b-c+a=0 -&gt; b=c-a</code></li>
<li>Second: <code>2b+a-c=2 -&gt; c=2b+a-2 -&gt; c=2(c-a)+a-2 -&gt; c=2c-2a+a-2 -&gt; c=a+2</code></li>
<li>Third: <code>5c-b-a=12 -&gt; 5(a+2)-(c-a)-a=12 -&gt; 5a+10-(a+2)+a-a=12 -&gt; 5a-a-2=2 -&gt; 4a=4 -&gt; a=1</code></li>
</ul>
<p>Now that we know <code>a=1</code> we could solve the rest <code>c=a+2=3</code> and <code>b=c-a=2</code>. And we have from 3 random equations regenerated the original fragments and could now recreate the original object. </p>
<p>The redundancy and reliability in such system comes in the form of creating (more than needed) equations and storing them. As shown these equations in any random order could recreate the original fragments and therefore
redundancy comes in at a much lower overhead.</p>
<h3 id="example-of-164"><a class="header" href="#example-of-164">Example of 16/4</a></h3>
<p><img src="technology/qsss/img/quantumsafe_storage_algo.jpg" alt="" /></p>
<p>Each object is fragmented into 16 parts. So we have 16 original fragments for which we need 16 equations to mathematically describe them. Now let's make 20 equations and store them dispersedly on 20 devices. To recreate the original object we only need 16 equations, the first 16 that we find and collect which allows us to recover the fragment and in the end the original object. We could lose any 4 of those original 20 equations.</p>
<p>The likelihood of losing 4 independent, dispersed storage devices at the same time is very low. Since we have continuous monitoring of all of the stored equations, we could create additional equations immediately when one of them is missing, making it an auto-regeneration of lost data and a self-repairing storage system. The overhead in this example is 4 out of 20 which is a mere <strong>20%</strong> instead of (up to) <strong>400%.</strong></p>
<h3 id="content-distribution-policy-1050"><a class="header" href="#content-distribution-policy-1050">Content distribution Policy (10/50)</a></h3>
<p>This system can be used as backend for content delivery networks.</p>
<p>Imagine a movie being stored on 60 locations from which we can loose 50 at the same time.</p>
<p>If someone now wants to download the data the first 10 locations who answer fastest will provide enough of the data parts to allow the data to be rebuild.</p>
<p>The overhead here is much more compared to previous example but stil order of magnitude lower compared to other cdn systems.</p>
<h3 id="datarot"><a class="header" href="#datarot">Datarot</a></h3>
<blockquote>
<p>Datarot cannot happen on this storage system</p>
</blockquote>
<p>Fact that data storage degrades over time and becomes unreadable, on e.g. a harddisk.
The storage system provided by ThreeFold intercepts this silent data corruption, making that it can pass by unnotified.</p>
<blockquote>
<p>see also https://en.wikipedia.org/wiki/Data_degradation</p>
</blockquote>
<h3 id="zero-knowledge-proof"><a class="header" href="#zero-knowledge-proof">Zero Knowledge Proof</a></h3>
<p>The quantum save storage system is zero knowledge proof compliant. The storage system is made up / split into 2 components:  The actual storage devices use to store the data (ZDB's) and the Quantum Safe Storage engine. </p>
<p><img src="technology/qsss/img/qss_system.jpg" alt="" /></p>
<p>The zero proof knowledge compliancy comes from the fact the all the physical storage nodes (3nodes) can proof that they store a valid part of what data the quantum safe storage engine (QSSE) has stored on multiple independent devices.  The QSSE can validate that all the QSSE storage devices have a valid part of the original information.  The storage devices however have no idea what the original stored data is as they only have a part (description) of the origina data and have no access to the original data part or the complete origal data objects.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="tfgrid-compute-layer"><a class="header" href="#tfgrid-compute-layer">TFGrid Compute Layer</a></h2>
<p><img src="technology/primitives/compute/img/tfgrid_compute_.jpg" alt="" /></p>
<p>We are more than just Container or VM technology, see <a href="technology/primitives/compute/../../primitives/compute/beyond_containers.html">our Beyond Container Document</a>.</p>
<p>For more information see <a href="technology/primitives/compute/../../zos/zos_toc.html">ZeroOS</a></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="mermaid.min.js"></script>
        <script type="text/javascript" src="mermaid-init.js"></script>

        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </body>
</html>
